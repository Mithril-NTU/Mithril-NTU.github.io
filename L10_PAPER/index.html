	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.15" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> L10_PAPER &middot; Daniel Liu </title>

  
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/poole.css">
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/syntax.css">
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Daniel Liu" />
</head>

	<body class="">
		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://mithril-ntu.github.io/"><h1>Daniel Liu</h1></a>
      <p class="lead">
       This is Daniel&#39;s Blog. 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
        <li><a href="/HD_LBP/"> HD_LBP </a></li>
      
        <li><a href="/L10_PAPER/"> L10_PAPER </a></li>
      
        <li><a href="/L11_PAPER/"> L11_PAPER </a></li>
      
        <li><a href="/L12_PAPER/"> L12_PAPER </a></li>
      
        <li><a href="/L13_PAPER/"> L13_PAPER </a></li>
      
        <li><a href="/L2_PAPER/"> L2_PAPER </a></li>
      
        <li><a href="/L3_PAPER/"> L3_PAPER </a></li>
      
        <li><a href="/L4_PAPER/"> L4_PAPER </a></li>
      
        <li><a href="/L5_PAPER/"> L5_PAPER </a></li>
      
        <li><a href="/L6_PAPER/"> L6_PAPER </a></li>
      
        <li><a href="/L7_PAPER/"> L7_PAPER </a></li>
      
        <li><a href="/L8_PAPER/"> L8_PAPER </a></li>
      
        <li><a href="/welcome/"> Welcome </a></li>
      
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>L10_PAPER</h1>
			  <span class="post-date">Wed, May 18, 2016</span>
			      

<h3 id="arxiv-16:2dfd18626dee5ce385c6309bd394bdc4">arXiv‘16</h3>

<h3 id="faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks:2dfd18626dee5ce385c6309bd394bdc4">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</h3>

<h4 id="introduction:2dfd18626dee5ce385c6309bd394bdc4">Introduction</h4>

<p>Fast-rcnn is relied on region proposal methods like Selective Search and EdgeBox, which are time-consuming and thus become computational bottleneck for real-time detection. So this paper introduces the Region Proposal Network (RPN) for object region proposal and merges it with Fast-rcnn by sharing their conv layers. This method turns out to be highly cost-efficient for practical usage and effective for accuracy improvement.</p>

<h4 id="faster-rcnn-framework:2dfd18626dee5ce385c6309bd394bdc4">Faster RCNN framework</h4>

<p>The main framework of this work is shown below:
<img src="/L10/Screen%20Shot%202016-05-18%20at%203.51.23%20PM.png" alt="" /></p>

<h5 id="region-proposal-network:2dfd18626dee5ce385c6309bd394bdc4">Region Proposal Network</h5>

<p>This RPN takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.
Details is shown below:
<img src="/L10/Screen%20Shot%202016-05-18%20at%204.22.52%20PM.png" alt="" />
The RPN firstly gets a lower-dimension feature from the feature map of the last shared conv layer by using a 3*3 spatial window. Then this feature is fed into 2 FC layers: a box-regression layer (<strong>reg</strong>) and a box-classification layer (<strong> cls</strong>).
The <strong>reg</strong> layer is for rectangular object proposals. The RPN uses k “anchor” boxes for region proposals associated with different scales and aspect ratios. (In this paper, the authors use 3 scales and 3 aspect ratios, that is, 9 anchors totally.) Then after computation, the <strong>reg</strong> layer outputs 4 parameters for each box which encode the coordinates of these boxes.
The <strong>cls</strong> layer is a binary classifier which can tell wether an object exists.
The loss function of this network is shown below:
<img src="/L10/Screen%20Shot%202016-05-18%20at%204.44.52%20PM.png" alt="" />
The first term is the log loss over two classes while the second term is the robust loss function (smooth L1) defined in [1]. The two terms are normalized by N_cls and N_reg and weighted by a balancing parameter λ.</p>

<h5 id="sharing-features-for-rpn-and-fast-r-cnn:2dfd18626dee5ce385c6309bd394bdc4">Sharing Features for RPN and Fast R-CNN</h5>

<p>The authors discuss 3 three ways for training networks with features shared:
1. Alternating training:
    Train RPN and Fast-rcnn in turn iteratively.
2. Approximate joint training:
    Train 2 network together by ignore the derivative w.r.t. the proposal boxes’ coordinates that are also network responses, so is approximate.
3. Non-approximate joint training:
    Train 2 network together but don’t ignore the derivative w.r.t. the proposal boxes’ coordinates. However, an RoI pooling layer is not differentiable w.r.t. the box coordinates.
In this paper, the authors adopt a pragmatic 4-step training algorithm in the end:
1. Train the RPN;
2. Train Fast R-CNN using the proposals generated by the step-1 RPN;
3. Use the detector network to initialize RPN training but fix the shared convolutional layers and only fine-tune the layers unique to RPN;
4. Keep the shared convolutional layers fixed and fine-tune the unique layers of Fast R-CNN.</p>

<h4 id="experiments:2dfd18626dee5ce385c6309bd394bdc4">Experiments</h4>

<h5 id="high-accuracy:2dfd18626dee5ce385c6309bd394bdc4">High accuracy</h5>

<p><img src="/L10/Screen%20Shot%202016-05-18%20at%205.01.28%20PM.png" alt="" /></p>

<h5 id="high-efficience:2dfd18626dee5ce385c6309bd394bdc4">High efficience</h5>

<p><img src="/L10/Screen%20Shot%202016-05-18%20at%205.02.33%20PM.png" alt="" /></p>

<h5 id="good-performance-of-rpn-compared-to-ss-and-eb:2dfd18626dee5ce385c6309bd394bdc4">Good performance of RPN compared to SS and EB</h5>

<p><img src="/L10/Screen%20Shot%202016-05-18%20at%205.03.09%20PM.png" alt="" /></p>

<h4 id="reference:2dfd18626dee5ce385c6309bd394bdc4">Reference</h4>

<p>[1] R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.</p>

			</div>

			
		</div>

  </body>
</html>
