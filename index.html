<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.15" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Daniel Liu &middot; Daniel Liu </title>

  
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/poole.css">
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/syntax.css">
  <link rel="stylesheet" href="http://mithril-ntu.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="http://Mithril-NTU.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Daniel Liu" />
</head>

<body class="">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="http://mithril-ntu.github.io/"><h1>Daniel Liu</h1></a>
      <p class="lead">
       This is Daniel&#39;s Blog. 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
        <li><a href="/HD_LBP/"> HD_LBP </a></li>
      
        <li><a href="/L2_PAPER/"> L2_PAPER </a></li>
      
        <li><a href="/L3_PAPER/"> L3_PAPER </a></li>
      
        <li><a href="/L4_PAPER/"> L4_PAPER </a></li>
      
        <li><a href="/L5_PAPER/"> L5_PAPER </a></li>
      
        <li><a href="/L6_PAPER/"> L6_PAPER </a></li>
      
        <li><a href="/L7_PAPER/"> L7_PAPER </a></li>
      
        <li><a href="/welcome/"> Welcome </a></li>
      
    </ul>

    <p>&copy; 2016. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L7_PAPER/">
        L7_PAPER
      </a>
    </h1>

    <span class="post-date">Tue, Apr 19, 2016</span>

    

<h3 id="nips-14:9c4afc3940f2cf7cbf85c1866eccd57b">NIPS‚Äò14</h3>

<h3 id="two-stream-convolutional-networks-for-action-recognition-in-videos:9c4afc3940f2cf7cbf85c1866eccd57b">Two-Stream Convolutional Networks for Action Recognition in Videos</h3>

<h4 id="introduction:9c4afc3940f2cf7cbf85c1866eccd57b">Introduction</h4>

<p>The main contribution of this paper can be divided into 3 parts: <strong>a)</strong> Propose a new two stream(spatial and temporal streams) ConvNet architecture for action recognition; <strong>b)</strong> Prove that a ConvNet trained on multi-frame dense optical flow can achieve good performance even if there are little training data; <strong>c)</strong> Propose multi-task learning which can overcome increase training data and improve the performance on both tasks.</p>

<p>This 2 stream ConvNet architecture is based on the hypothesis that the human visual cortex contains two pathways: the ventral stream (which performs object recognition) and the dorsal stream (which recognises motion).</p>

<h4 id="two-stream-convnet-architecture:9c4afc3940f2cf7cbf85c1866eccd57b">Two stream ConvNet architecture</h4>

<p><img src="/L7/Screen%20Shot%202016-04-19%20at%203.24.33%20PM.png" alt="" />
The authors divide the video into 2 streams which carry different information and then are processed by their corresponding ConvNets. The softmax output of these two ConvNets are combined by late fusion.</p>

<h5 id="spatial-stream-convnet:9c4afc3940f2cf7cbf85c1866eccd57b">Spatial stream ConvNet</h5>

<p>This ConvNet processes still images extracted from video frames which carry clues about objects and scenes associated with human actions. The author makes good use of the network pre-trainded on the ImageNet challenge dataset to improve this ConvNet.</p>

<h5 id="optical-flow-convnets:9c4afc3940f2cf7cbf85c1866eccd57b">Optical flow ConvNets</h5>

<p>In this ConvNet, the only difference with the spatial ConvNet‚Äôs configurations is that the second normalisation layer is removed  from the latter to reduce memory consumption.
The authors configure different inputs for this new model: <strong>Optical flow stacking</strong>, <strong>Trajectory stacking</strong> and <strong>Bi-directional optical flow</strong>, which is shown below:
<img src="/L7/Screen%20Shot%202016-04-19%20at%203.41.26%20PM.png" alt="" />
Moreover, to compensate the camera motion, the authors also perform ‚Äú<strong>Mean flow subtraction</strong>‚Äù ‚Äî‚Äî from each displacement field <strong>d</strong> the author subtract its mean vector. This also is generally beneficial to perform zero-centering of the network input, as it allows the model to better exploit the rectification non-linearities.</p>

<p>The authors think that the hand-crafted features in prior art can be obtained from the displacement field input using a single convolutional layer, which means that they can be generalised by their ConvNets, so as to the HMAX models.</p>

<h4 id="multi-task-learning:9c4afc3940f2cf7cbf85c1866eccd57b">Multi-task learning</h4>

<p>Unlike the spatial ConvNet which can be pre-trained on ImageNet Challenge dataset, the amount of the data that can be used in the temporal ConNet is less. In order to increase data, the authors set 2 softmax classification layer on top of the last FC layer for different dataset. The overall training loss is computed as the sum of the individual tasks‚Äô losses. Therefore, the whole ConvNet can make use of different datasets for different tasks.</p>

<h4 id="experiments:9c4afc3940f2cf7cbf85c1866eccd57b">Experiments</h4>

<p>The architecture of 2 stream ConvNet is based on VGG and, for the spatial ConvNet, pre-trained on ImageNet ILSVRC-2012. The datasets used for evaluation are UCF-101 and HMDB-51.</p>

<p><strong>Firstly</strong>, the authors compare different settings on the spatial and temporal ConvNet. The results are shown below:
<img src="/L7/Screen%20Shot%202016-04-19%20at%204.17.33%20PM.png" alt="" />
It turns out that fine-tuning on the whole pre-trained model can lead to the best performance, but the improvement is too slightly and  time-consuming. So the authors opt for only training the last layer in following experiments.
On the other hand, for the temporal ConvNet, using the Optical flow stacking as input is a good choice and it also shows that increasing the number of input flows(L) and performing Mean flow subtraction make a great difference.</p>

<p><strong>Secondly</strong>, the authors compare different methods to increase the training dataset size: (i) fine-tuning a temporal network pre-trained on UCF-101; (ii) adding 78 classes from UCF-101, which are manually selected so that there is no intersection between these classes and the native HMDB-51 classes; (iii) using the multi-task formulation to learn a video representation, shared between the UCF-101 and HMDB-51 classification tasks. The results is shown below:
<img src="/L7/Screen%20Shot%202016-04-19%20at%204.33.41%20PM.png" alt="" /></p>

<p><strong>Thirdly</strong> ,different fusion methods for combining the softmax scores of the spatial and temporal ConvNet are tested.
<img src="/L7/Screen%20Shot%202016-04-19%20at%204.35.58%20PM.png" alt="" />
Obviously, the combination of the two nets further improves the results, especially when using SVM for fusion.</p>

<p><strong>Finally</strong>, by comparing with result in prior art, the architecture proposed in this paper shows its competitive performance.
<img src="/L7/Screen%20Shot%202016-04-19%20at%204.37.31%20PM.png" alt="" /></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/HD_LBP/">
        HD_LBP
      </a>
    </h1>

    <span class="post-date">Sun, Apr 17, 2016</span>

    

<h3 id="cvpr-13:d8e10a38c5169f493ef99a37c63f86f5">CVPR‚Äô13</h3>

<h3 id="blessing-of-dimensionality-high-dimensional-feature-and-its-efficient-compression-for-face-verification:d8e10a38c5169f493ef99a37c63f86f5">Blessing of Dimensionality: High-dimensional Feature and Its Efficient Compression for Face Verification</h3>

<h4 id="apporach:d8e10a38c5169f493ef99a37c63f86f5">Apporach</h4>

<p>This paper has two main contributions:</p>

<h5 id="a-show-that-high-dimensionality-is-critical-to-high-performance:d8e10a38c5169f493ef99a37c63f86f5">(A)Show that high dimensionality is critical to high performance</h5>

<p>The author uses explicit shape regression to locate accurate landmarks on faces and then rectify similarity transformation based on five land- marks (eyes, nose, and mouth corners). With the location of these landmarks, the author extracts multi-scale patches around them and divide these patches into cells. Finally, these cells are coded by some local descriptors.
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.03.37%20AM.png" alt="" /></p>

<h5 id="b-propose-a-sparse-projection-method-named-rotated-sparse-regression:d8e10a38c5169f493ef99a37c63f86f5">(B)Propose a  sparse projection method, named rotated sparse regression</h5>

<p>Firstly, the author adopts PCA for feature dimension reduction and uses some supervised subspace learning methods like LDA or Joint Beyesian to extract discriminative information for face recognition and (potentially) further reduce the dimension.
Secondly, he uses propose a  sparse projection method, named rotated sparse regression, to learn a sparse linear projection.
This RSR(rotated sparse regression) is based on this sparse coding function:
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.12.47%20AM.png" alt="" />
By adding a rotation matrix R, this becomes to:
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.12.55%20AM.png" alt="" />
Though fixing R and B in turn, we can optimize them iteratively.
Finally, we can get a linear projection matrix B with additional freedom in rotation.
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.17.58%20AM.png" alt="" /></p>

<h4 id="experiment:d8e10a38c5169f493ef99a37c63f86f5">Experiment</h4>

<h5 id="the-high-dimensional-feature-is-better:d8e10a38c5169f493ef99a37c63f86f5">The High-dimensional feature is better</h5>

<p><img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.24.57%20AM.png" alt="" />
The author first compares their results with baseline feature extracted from regular grids to show that sampling at the landmarks leads to comparatively better performance, which indicates sampling at the landmarks effectively reduce the intra-personal geometric variations due to pose and expressions.
Then he compares different scale number and landmark number:
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.27.34%20AM.png" alt="" /></p>

<h5 id="large-scale-dataset-favors-high-dimensionality:d8e10a38c5169f493ef99a37c63f86f5">Large scale dataset favors high dimensionality</h5>

<p><img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.28.39%20AM.png" alt="" />
The author uses a new and larger dataset named WDRef to evaluate the performance of large scale dataset. It turns out that  high dimensionality plays an even more important role when the size of the training set becomes larger.</p>

<h5 id="high-dimensional-feature-with-unsupervised-learning:d8e10a38c5169f493ef99a37c63f86f5">High-dimensional feature with unsupervised learning</h5>

<p><img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.31.53%20AM.png" alt="" />
The high dimension feature also leads to high performance in unsupervised learning.</p>

<h5 id="compression-by-rotated-sparse-regression:d8e10a38c5169f493ef99a37c63f86f5">Compression by rotated sparse regression</h5>

<p>By varying the value of ùúÜ, the author compares the sparse regression and the rotated sparse regression under different sparsity.
<img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.34.41%20AM.png" alt="" />
RSR can reduce the cost of linear projection by 100 times with less than 0.1% accuracy drop.</p>

<h5 id="comparison-with-feature-selection:d8e10a38c5169f493ef99a37c63f86f5">Comparison with Feature Selection</h5>

<p><img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.35.48%20AM.png" alt="" />
The author compare the rotated sparse regression and two feature selection methods: backward greedy and structure sparsity.
This result verifies the effectiveness of the proposed method (RSR). It also indicates that the majority of dimensions in our high-dimensional feature are informative and complementary.</p>

<h5 id="comparison-with-the-state-of-the-art:d8e10a38c5169f493ef99a37c63f86f5">Comparison with the state-of-the-art</h5>

<p><img src="/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.40.25%20AM.png" alt="" /></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L6_PAPER/">
        L6_PAPER
      </a>
    </h1>

    <span class="post-date">Sun, Apr 10, 2016</span>

    

<h3 id="cvpr-05:c72d5234a95e9488aa0b579f888ff00d">CVPR‚Äò05</h3>

<h3 id="a-bayesian-hierarchical-model-for-learning-natural-scene-categories:c72d5234a95e9488aa0b579f888ff00d">A Bayesian Hierarchical Model for Learning Natural Scene Categories</h3>

<h4 id="introduction:c72d5234a95e9488aa0b579f888ff00d">Introduction</h4>

<p>The main problem in this paper is to learn and recognize natural scene categories without human annotation. According to previous work, it turns out that the key point to classify scenes is to use some intermediate representations. However, this kind of methods need tedious ,somewhat arbitrary and possibly sub-optimal, manual annotation. So this paper‚Äôs author proposes a new method which is incorporated with models used in texture classification. A scene image can be divide into many local regions(described by codewords) and these regions can be clustering to different themes and then to different categories.
The main framework is shown below:
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.28.57%20PM.png" alt="" />
1. Divide training images into different patches and represent these patches as part of different themes
2. Learns the theme distributions over categories as well as the codewords distribution over the themes without supervision, the author‚Äôs method is based on LDA.
3. Represent testing images with codewords and classify them to the label that gives the highest likelihood probability</p>

<h4 id="approach-details:c72d5234a95e9488aa0b579f888ff00d">Approach details</h4>

<h6 id="model-details:c72d5234a95e9488aa0b579f888ff00d">Model details</h6>

<p>The model we need to learn is shown below:
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.41.25%20PM.png" alt="" />
The meaning of notations in the above image is shown in red.</p>

<p>Based on this model, the joint probability of of a theme mixture œÄ, a set of N themes z, a set of N patches x and the category c is
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.45.39%20PM.png" alt="" />
where function Dir() is
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.45.48%20PM.png" alt="" />
According to this, we easily know that when x, Œò and Œ≤ is given, the probability of a scene class c is
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.51.52%20PM.png" alt="" />
where the last term is
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.56.33%20PM.png" alt="" />
Using Jensen‚Äôs inequality, we can bound this log likelihood as following:
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.57.15%20PM.png" alt="" />
By letting L(Œ≥,œÜ;Œ∏,Œ≤) denote the RHS of the above equation, we have:
<img src="/L6/Screen%20Shot%202016-04-10%20at%203.57.39%20PM.png" alt="" />
where the second term on the RHS of the above equation stands for the Kullback-Leibler distance of two probabil- ity densities.
Through iteratively estimating the variational parameters Œ≥ and œÜ and then estimating the model parameters Œ∏ and Œ≤ in turn, we can maxmize the log likelihood term log p(x|Œ∏, Œ≤, c).</p>

<h5 id="feature-details:c72d5234a95e9488aa0b579f888ff00d">Feature details</h5>

<p>The author extracts local regions from images in several ways:
<img src="/L6/Screen%20Shot%202016-04-10%20at%204.05.58%20PM.png" alt="" />
And then he represents these regions by 2 different features: normalized 11 √ó 11 pixel gray values and a 128‚àídim SIFT vector.
The codebook is learned by K-means algorithm.
The comparison result is:
<img src="/L6/Screen%20Shot%202016-04-10%20at%205.05.54%20PM.png" alt="" /></p>

<h4 id="experiments:c72d5234a95e9488aa0b579f888ff00d">Experiments</h4>

<p>The dataset used for the experiment contains 13 categories.
The classification result is shown below:
<img src="/L6/Screen%20Shot%202016-04-10%20at%204.12.50%20PM.png" alt="" />
We can see that using both the best and second best choices, the mean categorization result is up to 82.3%. In the confusing table, it‚Äôs obvious that four indoor categories suffer severe confusion with each other. The main reason is that the distribution of both the themes and the codewords of these four indoor categories is similar:
<img src="/L6/Screen%20Shot%202016-04-10%20at%204.16.50%20PM.png" alt="" />
This also can be proved by their relationship shown in the dendrogram:
<img src="/L6/Screen%20Shot%202016-04-10%20at%205.04.14%20PM.png" alt="" /></p>

<p>The author also compares the other things:
<img src="/L6/Screen%20Shot%202016-04-10%20at%205.04.51%20PM.png" alt="" /></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L5_PAPER/">
        L5_PAPER
      </a>
    </h1>

    <span class="post-date">Mon, Mar 21, 2016</span>

    

<h3 id="science-00:d6a9e0d3e47c951cd76ecbc765d293e7">Science‚Äô00</h3>

<h3 id="nonlinear-dimensionality-reduction-by-locally-linear-embedding:d6a9e0d3e47c951cd76ecbc765d293e7">Nonlinear Dimensionality Reduction by Locally Linear Embedding</h3>

<h4 id="abstract:d6a9e0d3e47c951cd76ecbc765d293e7">Abstract</h4>

<h5 id="problem:d6a9e0d3e47c951cd76ecbc765d293e7">Problem:</h5>

<p>How to discover compact representations of high-dimensional data, which can reserve the manifold structure of original data.</p>

<h5 id="solution:d6a9e0d3e47c951cd76ecbc765d293e7">Solution:</h5>

<p>Locally Linear Embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs</p>

<h4 id="summary:d6a9e0d3e47c951cd76ecbc765d293e7">Summary</h4>

<h5 id="intuition:d6a9e0d3e47c951cd76ecbc765d293e7">Intuition</h5>

<p>The geometric intuition of LLE is that the author expects each data point and its neighbors to lie on or close to a locally linear patch of the manifold, so he characterize the local geometry of these patches by linear coefficients that reconstruct each data point from its neighbors.</p>

<h5 id="methods:d6a9e0d3e47c951cd76ecbc765d293e7">Methods</h5>

<p>Based on this intuition, the author defines a cost function:
<img src="/L5/Screen%20Shot%202016-03-21%20at%2011.11.06%20PM.png" alt="" />
This cost function subjects to 2 constraints:
(1) enforcing W_ij   0 if X_j does not belong to the set of neighbors of X_i;
(2) <img src="/L5/Screen%20Shot%202016-03-21%20at%2011.13.30%20PM.png" alt="" />
Through minimising the cost function, which is a least-squares problem, we can get weights that reflect intrinsic geometric properties of the data that are invariant to rotations, rescalings, and translations of that data point and its neigh- bors.</p>

<p>Then the author maps the high dimension data to a low dimension vector representing global internal coordinates on the manifold based on the weights we get above. To get the best result, the author minimises a cost function as following:
<img src="/L5/Screen%20Shot%202016-03-21%20at%2011.34.36%20PM.png" alt="" /></p>

<p>The procedures described above can be shown in the figure below:
<img src="/L5/Screen%20Shot%202016-03-21%20at%2011.37.07%20PM.png" alt="" /></p>

<h5 id="examples:d6a9e0d3e47c951cd76ecbc765d293e7">Examples</h5>

<p>Two-dimensional embeddings based on LLE of faces and words are shown:
<img src="/L5/Screen%20Shot%202016-03-21%20at%2011.39.17%20PM.png" alt="" />
<img src="/L5/Screen%20Shot%202016-03-21%20at%2011.39.05%20PM.png" alt="" /></p>

<h5 id="advantages:d6a9e0d3e47c951cd76ecbc765d293e7">Advantages</h5>

<p>(1) Guarantee the global optimality or convergence;
(2) Involve less free parameters, which means simply tuning;
(3) The optimizations of LLE are especially tractable;
(4) Scales well with the intrinsic manifold dimensionality, d, and do not require adiscretized gridding of the embedding space.
(5) Avoid the need to solve large dynamic programming problems, and tend to accumulate very sparse matrices, whose structure can be exploited for savings in time and space.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L4_PAPER/">
        L4_PAPER
      </a>
    </h1>

    <span class="post-date">Wed, Mar 16, 2016</span>

    

<h3 id="icml-09:6e9caa20e3502a65896320b3023d9e3d">ICML‚Äò09</h3>

<h3 id="online-dictionary-learning-for-sparse-coding:6e9caa20e3502a65896320b3023d9e3d">Online Dictionary Learning for Sparse Coding</h3>

<h4 id="abstract:6e9caa20e3502a65896320b3023d9e3d">Abstract</h4>

<h5 id="problem:6e9caa20e3502a65896320b3023d9e3d">Problem</h5>

<p>To learn basis elements, of which we model data vectors as linear combination, especially on large-scale datasets</p>

<h5 id="solution:6e9caa20e3502a65896320b3023d9e3d">Solution</h5>

<p>Propose a online optimization algorithm for dictionary learning based on stochastic approximations
(A proof of convergence is provided.)</p>

<h4 id="introduction:6e9caa20e3502a65896320b3023d9e3d">Introduction</h4>

<p>Learning the dictionary instead of using predefined off-the-shelf bases has been shown to dramatically improve signal reconstruction.But, todays‚Äô methods can‚Äôt handle large-scale datasets.
To address this problem, the author proposes a method based on stochastic approximation that process one element of the training set at a time. It is shown to be go further and exploit the specific structure of sparse coding.</p>

<h4 id="paper-structure:6e9caa20e3502a65896320b3023d9e3d">Paper structure</h4>

<pre><code>2. Problem Statement

3. Online Dictionary Learning
3.1. Algorithm Outline
3.2. Sparse Coding
3.3. Dcitionary Update
3.4. Optimizing the Algorithm

4. Convergence Analysis
4.1. Assumptions
4.2. Main Result and Proof Sketches

5. Experimental Validation
5.1. Performance Evaluation
5.2. Application to Inpainting

6. Discussion
</code></pre>

<h4 id="2-problem-statement:6e9caa20e3502a65896320b3023d9e3d">2. Problem Statement</h4>

<p>Training set X = [x_1,x_2,‚Ä¶,x_n] in R^(m*n).
The empirical cost function:
<img src="/L4/Screen%20Shot%202016-03-16%20at%203.51.43%20PM.png" alt="" />
where D is in R^(m√ók).
The author defines l(x,D) as following:
<img src="/L4/Screen%20Shot%202016-03-16%20at%203.53.11%20PM.png" alt="" />
where Œª is a l1 regularization parameter to yield a sparse solution for Œ±.
To prevent D from being arbitrarily large, the author constraints D in a convex set C:
<img src="/L4/Screen%20Shot%202016-03-16%20at%203.56.02%20PM.png" alt="" />
In conclusion, the problem is stated as below:
<img src="/L4/Screen%20Shot%202016-03-16%20at%203.57.15%20PM.png" alt="" />
As this problem has 2 variables, the author tries to solve the problem by alternating between the 2 variables, minimizing over one while keeping the other one fixed.</p>

<p>In particular, given a finite training set, one should not spend too much effort on accurately minimizing the empirical cost, since it is only an approximation of the expected cost.
The expected cost is shown as following:
<img src="/L4/Screen%20Shot%202016-03-16%20at%204.05.02%20PM.png" alt="" /></p>

<p>To address this problem, the author proposes a new method based on stochastic approximations but exploits the specific structure of the problem to solve it(especially no need to tune the learning rate œÅ).
SG update is shown below:
<img src="/L4/Screen%20Shot%202016-03-16%20at%204.14.17%20PM.png" alt="" /></p>

<h4 id="3-online-dictionary-learning:6e9caa20e3502a65896320b3023d9e3d">3. Online Dictionary Learning</h4>

<h5 id="3-1-algorithm-outline:6e9caa20e3502a65896320b3023d9e3d">3.1. Algorithm Outline</h5>

<p><img src="/L4/Screen%20Shot%202016-03-16%20at%204.41.44%20PM.png" alt="" />
D_t is computed by minimizing over C the function:
<img src="/L4/Screen%20Shot%202016-03-16%20at%204.42.45%20PM.png" alt="" />
This quadratic function acts as a surrogate for f_t, so D_t can be obtained efficiently using D_(t-1) as warm start</p>

<h5 id="3-2-sparse-coding:6e9caa20e3502a65896320b3023d9e3d">3.2. Sparse Coding</h5>

<p>Because of the high correlation between the columns in D, the author chooses the LARS-Lasso algorithm to get Œ±.</p>

<h5 id="3-3-dictionary-update:6e9caa20e3502a65896320b3023d9e3d">3.3. Dictionary Update</h5>

<p><img src="/L4/Screen%20Shot%202016-03-16%20at%204.45.46%20PM.png" alt="" />
The algorithm for updating the dictionary uses block-coordinate descent with warm restarts, and one of its main advantages is that it is parameter-free and does not require any learning rate tuning, which can be difficult in a constrained optimization setting.</p>

<h5 id="3-4-optimizing-the-algorithm:6e9caa20e3502a65896320b3023d9e3d">3.4. Optimizing the Algorithm</h5>

<p>Some practical tricks:
1) Handling Fixed-size Datasets:
If the same signal x is drawn again at time t &gt; t_0, one would like to remove the ‚Äúold‚Äù information concerning x from A_t and B_t ‚Äî- that is,
<img src="/L4/Screen%20Shot%202016-03-16%20at%205.08.31%20PM.png" alt="" />
2) Mini-Batch Extension
For efficiency, the author uses mini-Batch(Œ∑ signals) in each iteration.
<img src="/L4/Screen%20Shot%202016-03-16%20at%205.10.38%20PM.png" alt="" />
3) Purging the dictionary from Unused Atoms
Replace unused atoms during the optimization by elements of the training set</p>

<h4 id="4-convergence-analysis:6e9caa20e3502a65896320b3023d9e3d">4. Convergence Analysis</h4>

<h5 id="4-1-assumptions:6e9caa20e3502a65896320b3023d9e3d">4.1. Assumptions</h5>

<p>1) The data admits a bounded probability density p with compact support K.
2) The quadratic surrogate functions fÀÜ_t are strictly convex with lower-bounded Hessians.
3) A sufficient uniqueness condition of the sparse coding solution is verified.</p>

<h5 id="4-2-main-result-and-proof-sketches:6e9caa20e3502a65896320b3023d9e3d">4.2. Main Result and Proof Sketches</h5>

<p><img src="/L4/Screen%20Shot%202016-03-16%20at%208.29.55%20PM.png" alt="" />
<img src="/L4/Screen%20Shot%202016-03-16%20at%208.30.06%20PM.png" alt="" /></p>

<h4 id="5-experimental-validation:6e9caa20e3502a65896320b3023d9e3d">5. Experimental Validation</h4>

<h5 id="5-1-performance-evaluation:6e9caa20e3502a65896320b3023d9e3d">5.1. Performance Evaluation</h5>

<p><img src="/L4/Screen%20Shot%202016-03-16%20at%208.33.07%20PM.png" alt="" /></p>

<h5 id="5-2-application-to-inpainting:6e9caa20e3502a65896320b3023d9e3d">5.2. Application to Inpainting</h5>

<p><img src="/L4/Screen%20Shot%202016-03-16%20at%208.34.40%20PM.png" alt="" /></p>

<h4 id="6-discussion:6e9caa20e3502a65896320b3023d9e3d">6. Discussion</h4>

<p>Advantage of this method:</p>

<p>1) Significantly faster than batch alternatives on large datasets</p>

<p>2) Not require learning rate tuning like regular stochastic gradient descent methods</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L3_PAPER/">
        L3_PAPER
      </a>
    </h1>

    <span class="post-date">Wed, Mar 16, 2016</span>

    

<h3 id="cvpr-11:f809bb8f2a579f5c8d55fa3650d344a6">CVPR‚Äò11</h3>

<h3 id="iterative-quantization-a-procrustean-approach-to-learning-binary-codes:f809bb8f2a579f5c8d55fa3650d344a6">Iterative Quantization: A Procrustean Approach to Learning Binary Codes</h3>

<h4 id="abstract:f809bb8f2a579f5c8d55fa3650d344a6">Abstract</h4>

<h5 id="problem:f809bb8f2a579f5c8d55fa3650d344a6">Problem:</h5>

<p>Learn similarity-preserving binary codes for efficient retrieval  in large scale image collections</p>

<h5 id="solution:f809bb8f2a579f5c8d55fa3650d344a6">Solution:</h5>

<p>An alternating minimization scheme(ITQ, Iterative Quantization): Finding a a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube
ITQ can be used with unsupervised or supervised data embeddings</p>

<h4 id="introduction:f809bb8f2a579f5c8d55fa3650d344a6">Introduction</h4>

<p>Encode high-dimensional image descriptor as compact binary strings. This codes should have 3 properties: 1) short enough for storing large datasets, 2) map similar images to binary codes with a low Hamming distance, 3) encode new images efficiently
The author‚Äôs approach is like following:
PCA -&gt; Apply a random orthogonal transformation(counteract the variance of different PCA directions) ‚Äî&gt; ITQ for refining the initial orthogonal transformation to minimize quantization error</p>

<h4 id="paper-structure:f809bb8f2a579f5c8d55fa3650d344a6">Paper structure</h4>

<pre><code>2. Unsupervised Code Learning 
2.1. Dimensionality Reduction
2.2. Binary Quantization

3. Evaluation of Unsupervised Code Learning 
3.1. Datasets
3.2. Protocols and Baseline Methods
3.3. Results on CIFAR Dataset
3.4. Results on 580000 Tiny Images

4. Leveraging Label Information

5. Discussion
</code></pre>

<h4 id="2-unsupervised-code-learning:f809bb8f2a579f5c8d55fa3650d344a6">2. Unsupervised Code Learning</h4>

<p>Procedure:
1) Apply linear dimensionality reduction to the data(PCA)
2) Perform binary quantization</p>

<h5 id="2-1-dimensionality-reduction:f809bb8f2a579f5c8d55fa3650d344a6">2.1. Dimensionality Reduction</h5>

<p>To maximize the variance approximately, we get the following objective function:
<img src="/L3/Screen%20Shot%202016-03-15%20at%2011.53.58%20PM.png" alt="" />
This is the same as PCA, so we get W by taking the top c eigenvectors of the data covariance matrix  X^TX</p>

<h5 id="2-2-binary-quantization:f809bb8f2a579f5c8d55fa3650d344a6">2.2. Binary Quantization</h5>

<p>To minimize the quantization loss, we get:
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.09.58%20AM.png" alt="" />
where ||.||_F is the Frobenius norm and R is some orthogonal c*c matrix.
The author initializes the R as a random orthogonal matrix. Then     adopt the ITQ procedure:
1) Fix R and update B:
Expanding the formulation above, we have
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.24.50%20AM.png" alt="" />
Minimizing this is equivalent to maximize:
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.26.09%20AM.png" alt="" />
2) Fix B and update R:
a) Compute the SVD of the c*c matrix B^TV as SŒ©S‚Äô^T
b) Let R = S‚ÄôS^T
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.29.23%20AM.png" alt="" /></p>

<h4 id="3-evaluation-of-unsupervised-code-learning:f809bb8f2a579f5c8d55fa3650d344a6">3. Evaluation of Unsupervised Code Learning</h4>

<h5 id="3-1-datasets:f809bb8f2a579f5c8d55fa3650d344a6">3.1. Datasets</h5>

<p>1) CIFAR dataset;
2)580000 Tiny images</p>

<h5 id="3-2-protocols-and-baseline-methods:f809bb8f2a579f5c8d55fa3650d344a6">3.2. Protocols and Baseline Methods</h5>

<p>Protocols:
1) To evaluate performance of nearest neighbor search using Euclidean neighbors as ground truth
2) To evaluate the semantic consistency of codes produced by different methods by using class labels as ground truth.</p>

<p>Baseline methods:
1) LSH;
2) PCA-Direct;
3) PCA-RR;
4) SH;
5) SKLSH;
6) PCA-Nonorth</p>

<h5 id="3-3-results-on-cifar-dataset:f809bb8f2a579f5c8d55fa3650d344a6">3.3. Results on CIFAR Dataset</h5>

<p><img src="/L3/Screen%20Shot%202016-03-16%20at%2012.42.06%20AM.png" alt="" />
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.36.03%20AM.png" alt="" /></p>

<h5 id="3-4-results-on-580000-tiny-images:f809bb8f2a579f5c8d55fa3650d344a6">3.4. Results on 580000 Tiny Images</h5>

<p><img src="/L3/Screen%20Shot%202016-03-16%20at%2012.41.03%20AM.png" alt="" />
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.41.08%20AM.png" alt="" /></p>

<h4 id="4-leveraging-label-information:f809bb8f2a579f5c8d55fa3650d344a6">4. Leveraging Label Information</h4>

<p>This section shows how to refine the binary codes in a supervised setting using Canonical Correlation Analysis(CCA).
The goal of CCA is to find projection directions w_k and u_k for feature and label vectors to maximize the correlation between the projected data X*w_k and Y*u_k:
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.48.41%20AM.png" alt="" />
The author compares his method with a semi-supervised approach(SSH):
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.50.50%20AM.png" alt="" />
<img src="/L3/Screen%20Shot%202016-03-16%20at%2012.51.45%20AM.png" alt="" /></p>

<h4 id="5-discussion:f809bb8f2a579f5c8d55fa3650d344a6">5. Discussion</h4>

<p>Contributions of this paper:
1) Show that the performance of PCA-based binary coding schemes can be greatly improved by simply rotating the projected data.</p>

<p>2) Demonstrate an iterative quantization method for refining this rotation that is very natural and effective.</p>

<p>Limitation:
Use one bit per projected data dimension.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/L2_PAPER/">
        L2_PAPER
      </a>
    </h1>

    <span class="post-date">Mon, Mar 14, 2016</span>

    

<h3 id="cvpr-10:87f4392be42726eab66708aaabb6a62d">CVPR‚Äò10</h3>

<h3 id="aggregating-local-descriptors-into-a-compact-image-representation:87f4392be42726eab66708aaabb6a62d">Aggregating local descriptors into a compact image representation</h3>

<h4 id="abstract:87f4392be42726eab66708aaabb6a62d">Abstract</h4>

<h5 id="problem:87f4392be42726eab66708aaabb6a62d">Problem:</h5>

<p>To find an image search algorithm for large dataset with high accuracy and efficiency and low memory cost</p>

<h5 id="solution:87f4392be42726eab66708aaabb6a62d">Solution:</h5>

<ol>
<li>Aggregating local image descriptors into a vector with limited dimension</li>
<li>jointly reduce features‚Äô dimension and index them</li>
</ol>

<h4 id="introduction:87f4392be42726eab66708aaabb6a62d">Introduction</h4>

<p>Today‚Äôs approaches are hard to fulfil 3 constraints: the search accuracy, its efficiency and the memory usage. To overcome this, the authors propose a new approach called ‚ÄúVLAD(Vector of Locally Aggregated Descriptors)‚Äù.
It proposes an image representation that provides high search accuracy with reasonable vector dimensionality. Then it jointly optimising the trade-off between the dimensionality reduction and the indexation algorithm.</p>

<h4 id="paper-structure:87f4392be42726eab66708aaabb6a62d">Paper structure</h4>

<pre><code>2.Image vector representation
2.1.Bag of features
2.2.Fisher kernel
2.3.VLAD

3.From vectors to codes
3.1.Approximate nearest neighbour
3.2.Indexation-aware dimensionality reduction

4.Experiments
4.1.Evaluation datasets and local descriptor
4.2.Image vector representations
4.3.Reduction and indexation
4.4.Compare with the state of the art
4.5.Large scale experiments
</code></pre>

<h4 id="2-image-vector-representation:87f4392be42726eab66708aaabb6a62d">2. Image vector representation</h4>

<h5 id="2-1-bag-of-features:87f4392be42726eab66708aaabb6a62d">2.1. Bag of features</h5>

<p>Extract local features from images ‚Äî&gt; Group these features into k ‚Äúvisual words‚Äù(k-means clustering) ‚Äî&gt; Represent an image by a weighted and normalised histograms</p>

<h5 id="2-2-fisher-kernel:87f4392be42726eab66708aaabb6a62d">2.2 Fisher kernel</h5>

<p>Learn a parametric generative model from training data -&gt; Describe a image with the gradient in parameter space which means how the learnt model should be modified to better fit the observed data</p>

<h5 id="2-3-vlad:87f4392be42726eab66708aaabb6a62d">2.3. VLAD</h5>

<p>1) Use SIFT descriptors &amp; Learn k ‚Äúvisual words‚Äù ;
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.36.18%20PM.png" alt="" />
2) Compute VLADÔºõ
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.36.36%20PM.png" alt="" />
3) v is subsequently L2 -normalizedÔºõ
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.39.34%20PM.png" alt="" />
The whole computation is like following:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.40.27%20PM.png" alt="" /></p>

<h4 id="3-from-vectors-to-codes:87f4392be42726eab66708aaabb6a62d">3. From vectors to codes</h4>

<p>This problem is divided into 2 steps: 1) a projections that reduces the dimensionality of the vector and 2) a quantisation used to index the resulting vectors.</p>

<h5 id="3-1-approximate-nearest-neighbour:87f4392be42726eab66708aaabb6a62d">3.1. Approximate nearest neighbour</h5>

<p>ANN is an approach that embeds a vector into a binary space with  excellent accuracy and efficient memory usage. And it provides explicit approximation of the indexed vectors.The author uses the asymmetric distance computation(ADC) variant of this approach.
To find the Œ± nearest neighbours NN_Œ±(x) of x, we just need compute:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.29%20PM.png" alt="" />
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.44%20PM.png" alt="" />
To embed the vector x into a binary space, we first spill it into (x^1, ‚Ä¶ , x^m) of equal length D/m. Then a product of quantiser is :
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.37%20PM.png" alt="" />
The approximation between the original vector y and the quantiser is:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.53%20PM.png" alt="" /></p>

<h5 id="3-2-indexation-aware-dimensionality-reduction:87f4392be42726eab66708aaabb6a62d">3.2. Indexation-aware dimensionality reduction</h5>

<p>The author use PCA for dimensionality reduction. Mapping a vector x ‚àà R^D to x‚Äô=Mx ‚àà R^D‚Äô will lead to information loss:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.46.01%20PM.png" alt="" />
With quantisation, this becomes:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.47.14%20PM.png" alt="" />
IF D‚Äô is large, then Œµ_p(x) is limited and Œµ_q(x) is large. There is a trade-off on the number of D‚Äô.</p>

<p>May here can use some optimisation algorithms.</p>

<p>In addition, because PCA, the variance of the different components of x‚Äô is not balanced. So the author performs an orthogonal transformation after PCA: X‚Äô‚Äô = QX‚Äô = QMX‚Äô.
Q is chose in the form of a Householder matrix:
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.54.12%20PM.png" alt="" />
or is chose as a random orthogonal matrix.</p>

<h4 id="4-experiments:87f4392be42726eab66708aaabb6a62d">4.Experiments</h4>

<h5 id="4-1-evaluation-datasets-and-local-descriptor:87f4392be42726eab66708aaabb6a62d">4.1.Evaluation datasets and local descriptor</h5>

<p>3 datasets: 1) INRIA Holidays dataset; 2) UKB dataset; 3) 10M images collected from Flickr</p>

<h5 id="4-2-image-vector-representations:87f4392be42726eab66708aaabb6a62d">4.2.Image vector representations</h5>

<p><img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.02.31%20PM.png" alt="" /></p>

<h5 id="4-3-reduction-and-indexation:87f4392be42726eab66708aaabb6a62d">4.3.Reduction and indexation</h5>

<p>1) Balancing the variance
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.03.35%20PM.png" alt="" />
2) Choice of the projection subspace dimension
<img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.03.41%20PM.png" alt="" /></p>

<h5 id="4-4-compare-with-the-state-of-the-art:87f4392be42726eab66708aaabb6a62d">4.4.Compare with the state of the art</h5>

<p><img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.04.16%20PM.png" alt="" />
For the same memory usage, this paper‚Äôs method outperforms others!</p>

<h5 id="4-5-large-scale-experiments:87f4392be42726eab66708aaabb6a62d">4.5.Large scale experiments</h5>

<p><img src="/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.07.03%20PM.png" alt="" /></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://mithril-ntu.github.io/welcome/">
        Welcome
      </a>
    </h1>

    <span class="post-date">Sat, Mar 12, 2016</span>

    

<h3 id="first-blog:2cc7dc244eed4480e8b46c91e911e96b">First Blog</h3>

<p>Hello Everyone!</p>


<figure >
    
        <img src="/media/tn.png" />
    
    
    <figcaption>
        <h4>example</h4>
        
    </figcaption>
    
</figure>


  </div>
  
</div>
</div>

  </body>
</html>
