<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Liu</title>
    <link>http://Mithril-NTU.github.io/</link>
    <description>Recent content on Daniel Liu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 May 2016 18:38:01 +0800</lastBuildDate>
    <atom:link href="http://Mithril-NTU.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>L13_PAPER</title>
      <link>http://mithril-ntu.github.io/L13_PAPER/</link>
      <pubDate>Wed, 25 May 2016 18:38:01 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L13_PAPER/</guid>
      <description>

&lt;h3 id=&#34;spm-12:b68c34f0e3ae52331757a585238f5737&#34;&gt;SPM’12&lt;/h3&gt;

&lt;h3 id=&#34;deep-neural-networks-for-acoustic-modelling-in-speech-recognition:b68c34f0e3ae52331757a585238f5737&#34;&gt;Deep Neural Networks for Acoustic Modelling in Speech Recognition&lt;/h3&gt;

&lt;h4 id=&#34;introduction:b68c34f0e3ae52331757a585238f5737&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;The paper mainly provides an overview of the DBN-DNN system and compares its performance with the GMM-HMM system’s on some datasets. The recent successes of DBN-DNN in ASR system are proved in these experiments.&lt;/p&gt;

&lt;h4 id=&#34;architecture-of-dbn-dnn:b68c34f0e3ae52331757a585238f5737&#34;&gt;Architecture of DBN-DNN&lt;/h4&gt;

&lt;p&gt;The main architecture of DBN-DNN is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%205.41.27%20PM.png&#34; alt=&#34;&#34; /&gt;
The whole system is firstly pre-trained on stacked RBMs and then the weights in these different RBMs are used to initialize corresponding layers of hidden units in a DNN. In the end, the initialised DNN is fine-tuned by back-propagation and then used to output &lt;em&gt;“p(HMMstate|AcousticInput)”&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&#34;rbm:b68c34f0e3ae52331757a585238f5737&#34;&gt;RBM&lt;/h5&gt;

&lt;p&gt;Restricted Boltzmann machines(RBM) are trained to maximize the  expected log probability assigned to some training set V:
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.02.43%20PM.png&#34; alt=&#34;&#34; /&gt;
where the probability of a visible vector is
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.03.02%20PM.png&#34; alt=&#34;&#34; /&gt;
and RBM energy function(for binary input) is
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.03.08%20PM.png&#34; alt=&#34;&#34; /&gt;
For real-valued data, the RBM energy function is defined as
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.06.03%20PM.png&#34; alt=&#34;&#34; /&gt;
where σ_i is the standard deviation of the Gaussian noise of visible unit i.
The authors stack RBMs to be a multi-layer model and trained it by contrastive divergence (CD) algorithm.&lt;/p&gt;

&lt;h5 id=&#34;dnn:b68c34f0e3ae52331757a585238f5737&#34;&gt;DNN&lt;/h5&gt;

&lt;p&gt;The generative weights learned in DBN are directly applied to the initialization of DNN. Then the DNN is fine-tuned by back-propagation. The fine-tuned DNN is interfaced with an HMM.&lt;/p&gt;

&lt;h4 id=&#34;experiments:b68c34f0e3ae52331757a585238f5737&#34;&gt;Experiments&lt;/h4&gt;

&lt;h5 id=&#34;the-timit-core-test-set:b68c34f0e3ae52331757a585238f5737&#34;&gt;the TIMIT core test set&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.25.28%20PM.png&#34; alt=&#34;&#34; /&gt;
Table I compares DBN-DNNs with a variety of other methods on the TIMIT core test set.
Two kinds of feature are used in this experiment: MFCC and fbank. The good performance of “fbank” feature reveals that DBN-DNN models don’t require uncorrelated data. It is also obvious that DBN-DNNs outperforms GMM-HMMs.&lt;/p&gt;

&lt;h5 id=&#34;other-datasets-tasks:b68c34f0e3ae52331757a585238f5737&#34;&gt;Other datasets(tasks)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.32.01%20PM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L13/Screen%20Shot%202016-05-25%20at%206.32.13%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L11_PAPER</title>
      <link>http://mithril-ntu.github.io/L11_PAPER/</link>
      <pubDate>Mon, 23 May 2016 00:34:24 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L11_PAPER/</guid>
      <description>

&lt;h3 id=&#34;cvpr-14:9371427216152d03703fefa395f11c8c&#34;&gt;CVPR’14&lt;/h3&gt;

&lt;h3 id=&#34;deepface-closing-the-gap-to-human-level-performance-in-face-verification:9371427216152d03703fefa395f11c8c&#34;&gt;DeepFace: Closing the Gap to Human-Level Performance in Face Verification&lt;/h3&gt;

&lt;h4 id=&#34;introduction:9371427216152d03703fefa395f11c8c&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;This paper uses a two-phase alignment methods to preprocess raw images and then derives representations of these processed images from a nine-layer deep neural network. State of art results are achieved in the LFW dataset.&lt;/p&gt;

&lt;h4 id=&#34;face-alignment:9371427216152d03703fefa395f11c8c&#34;&gt;Face alignment&lt;/h4&gt;

&lt;p&gt;Since face is a 3D object, the authors believe that employing a 3D model for alignment is the right way. Their alignment method, which can be divided into 2 phases, is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-22%20at%2011.27.15%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;first-phase-2d-alignment:9371427216152d03703fefa395f11c8c&#34;&gt;First phase — 2D alignment&lt;/h5&gt;

&lt;p&gt;The authors combine LBP descriptor with a Support Vector Regressor (SVR) to train a simple six-fiducial-point detector and then use these 6 fiducial points to approximately scale, rotate and translate the image into six anchor locations. The resulted final 2D similarity transformation can assist to compensate for the in-plane rotation of faces in raw images.&lt;/p&gt;

&lt;h5 id=&#34;second-phase-3d-alignment:9371427216152d03703fefa395f11c8c&#34;&gt;Second phase — 3D alignment&lt;/h5&gt;

&lt;p&gt;For out-of-plane rotation, the authors use a generic 3D shape model and register a 3D affine camera, which are used to warp the 2D-aligned crop to the image plane of the 3D shape.
To achieve this, the authors firstly use a second SVR to generate 67 fiducial points in the 2D-aligned crops from the first phase. Then, the authors find an affine transformation matrix to fit the 3D model into the 2D image by solving a least squares problem. In the end, this 3D model is used to frontalize the 2D-aligned crops.&lt;/p&gt;

&lt;h4 id=&#34;representation:9371427216152d03703fefa395f11c8c&#34;&gt;Representation&lt;/h4&gt;

&lt;p&gt;A nine-layer deep neural network is trained to get a discriminative representation of face images:
&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.01.16%20AM.png&#34; alt=&#34;&#34; /&gt;
C1 and C3 are global convolutional layers, while L3, L4 and L5 are locally convolutional layers which apply different filters to different regions in the feature map. M2 is a max-pooling layer. The other 3 layers are 2 fully-connected layers and 1 softmax layer.
To reduce the sensitivity to illumination changes, the authors normalize the features to be between zero and one.&lt;/p&gt;

&lt;h4 id=&#34;verification-metric:9371427216152d03703fefa395f11c8c&#34;&gt;Verification Metric&lt;/h4&gt;

&lt;p&gt;To verify whether two input instances belong to the same class, the authors try 2 metric learning methods: 1) the weighted χ2 similarity; 2) the Siamese network.&lt;/p&gt;

&lt;h4 id=&#34;experiments:9371427216152d03703fefa395f11c8c&#34;&gt;Experiments&lt;/h4&gt;

&lt;h5 id=&#34;compare-different-training-dataset-sizes-and-depth-of-neural-network:9371427216152d03703fefa395f11c8c&#34;&gt;Compare different training dataset sizes and depth of neural network&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.17.31%20AM.png&#34; alt=&#34;&#34; /&gt;
The first column (Subsets of sizes 1.5K, 3K and 4K persons) indicates that the capacity of the network can well accommodate the scale of 3M training images.
The second column (the global number of samples in SFC to 10%, 20%, 50%) shows that the network would benefit from even larger datasets.
The third column (chopping off the C3 layer, the two local L4 and L5 layers, or all these 3 layers, referred respectively as DF-sub1, DF-sub2, and DF-sub3) verifies the necessity of network depth when training on a large face dataset.&lt;/p&gt;

&lt;h5 id=&#34;compare-different-inputs-network-architectures-and-other-state-of-art-methods:9371427216152d03703fefa395f11c8c&#34;&gt;Compare different inputs, network architectures and other state of art methods&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.22.49%20AM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.26.04%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;compare-on-the-ytf-dataset:9371427216152d03703fefa395f11c8c&#34;&gt;Compare on the YTF dataset&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.30.47%20AM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L11/Screen%20Shot%202016-05-23%20at%2012.30.54%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L12_PAPER</title>
      <link>http://mithril-ntu.github.io/L12_PAPER/</link>
      <pubDate>Thu, 19 May 2016 00:22:17 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L12_PAPER/</guid>
      <description>

&lt;h3 id=&#34;arxiv-16:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;arXiv’16&lt;/h3&gt;

&lt;h3 id=&#34;text-understanding-from-scratch:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Text Understanding from Scratch&lt;/h3&gt;

&lt;h4 id=&#34;introduction:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;This paper proposes a method based on temporal ConvNet which performs surprisingly without any knowledge of words, phrases, sentences and any other syntactic or semantic structures. The authors think that ConvNet operating on characters can learn the hierarchical representations of words, phrases and sentences by itself with a lot of data.&lt;/p&gt;

&lt;h4 id=&#34;convnet-model-design:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;ConvNet Model Design&lt;/h4&gt;

&lt;h5 id=&#34;key-modules:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Key modules&lt;/h5&gt;

&lt;p&gt;The key modules used in this paper are similar to what we used in Computer Vision.
The &lt;strong&gt;convolutional&lt;/strong&gt; layer is defined as the following equation:
&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.01.52%20PM.png&#34; alt=&#34;&#34; /&gt;
where d is the stride, c=k-d+1 is an offset constant, f(x) is kernel function, g(x) is input function and h(y) is the final output function.
The &lt;strong&gt;max-pooling&lt;/strong&gt; layer is defined as below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.06.39%20PM.png&#34; alt=&#34;&#34; /&gt;
The &lt;strong&gt;activation&lt;/strong&gt; function is &lt;strong&gt;h(x)=max{0,x}&lt;/strong&gt;, which is similar to the ReLu.&lt;/p&gt;

&lt;h5 id=&#34;character-quantization:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Character quantization&lt;/h5&gt;

&lt;p&gt;The authors first prescribe an alphabet of size m for the input language and then quantize each character using 1-of-m encoding. So, in the end, the input text becomes a l*m matrix, where l means the number of frames and m means the size of each frame. After this quantization, our binary encoding is similar to Braille which is assisting blind reading:
&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.18.06%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;model-design:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Model Design&lt;/h5&gt;

&lt;p&gt;The whole ConvNet framework of this paper is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.19.42%20PM.png&#34; alt=&#34;&#34; /&gt;
which has 6 Conv layers and 3 FC layers.&lt;/p&gt;

&lt;h5 id=&#34;data-augmentation:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Data augmentation&lt;/h5&gt;

&lt;p&gt;Synonym replacement with an English thesaurus has been used for data augmentation.&lt;/p&gt;

&lt;h4 id=&#34;experiments:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Experiments&lt;/h4&gt;

&lt;h5 id=&#34;performance-on-ontology-classification:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Performance on Ontology Classification&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.25.01%20PM.png&#34; alt=&#34;&#34; /&gt;
As we can see, a proper thesaurus augmentation works well for model generalisation.
&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.30.32%20PM.png&#34; alt=&#34;&#34; /&gt;
In the visualization, black (or white) indicates large negative (or positive) values, and gray indicates values near zero. It’s obvious that the ConvNet focuses more on letters than other characters, which means that it has learned something.&lt;/p&gt;

&lt;h5 id=&#34;performance-on-sentiment-analysis:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Performance on Sentiment Analysis&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.27.09%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;performance-on-topic-classification:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Performance on Topic Classification&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.27.42%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;performance-on-news-categorization-in-english:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Performance on News Categorization in English&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.28.07%20PM.png&#34; alt=&#34;&#34; /&gt;
The overfitting occurred in this result reveals that to achieve good text understanding results ConvNets require a large corpus.&lt;/p&gt;

&lt;h5 id=&#34;performance-on-news-categorization-in-chinese:b22ba9caa617c5be07c9c6fef21aa826&#34;&gt;Performance on News Categorization in Chinese&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L12/Screen%20Shot%202016-05-18%20at%209.28.18%20PM.png&#34; alt=&#34;&#34; /&gt;
Both in Chinese and English the method performs well and thus prove its generalisation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L10_PAPER</title>
      <link>http://mithril-ntu.github.io/L10_PAPER/</link>
      <pubDate>Wed, 18 May 2016 17:12:10 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L10_PAPER/</guid>
      <description>

&lt;h3 id=&#34;arxiv-16:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;arXiv‘16&lt;/h3&gt;

&lt;h3 id=&#34;faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/h3&gt;

&lt;h4 id=&#34;introduction:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Fast-rcnn is relied on region proposal methods like Selective Search and EdgeBox, which are time-consuming and thus become computational bottleneck for real-time detection. So this paper introduces the Region Proposal Network (RPN) for object region proposal and merges it with Fast-rcnn by sharing their conv layers. This method turns out to be highly cost-efficient for practical usage and effective for accuracy improvement.&lt;/p&gt;

&lt;h4 id=&#34;faster-rcnn-framework:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Faster RCNN framework&lt;/h4&gt;

&lt;p&gt;The main framework of this work is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%203.51.23%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;region-proposal-network:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Region Proposal Network&lt;/h5&gt;

&lt;p&gt;This RPN takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.
Details is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%204.22.52%20PM.png&#34; alt=&#34;&#34; /&gt;
The RPN firstly gets a lower-dimension feature from the feature map of the last shared conv layer by using a 3*3 spatial window. Then this feature is fed into 2 FC layers: a box-regression layer (&lt;strong&gt;reg&lt;/strong&gt;) and a box-classification layer (&lt;strong&gt; cls&lt;/strong&gt;).
The &lt;strong&gt;reg&lt;/strong&gt; layer is for rectangular object proposals. The RPN uses k “anchor” boxes for region proposals associated with different scales and aspect ratios. (In this paper, the authors use 3 scales and 3 aspect ratios, that is, 9 anchors totally.) Then after computation, the &lt;strong&gt;reg&lt;/strong&gt; layer outputs 4 parameters for each box which encode the coordinates of these boxes.
The &lt;strong&gt;cls&lt;/strong&gt; layer is a binary classifier which can tell wether an object exists.
The loss function of this network is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%204.44.52%20PM.png&#34; alt=&#34;&#34; /&gt;
The first term is the log loss over two classes while the second term is the robust loss function (smooth L1) defined in [1]. The two terms are normalized by N_cls and N_reg and weighted by a balancing parameter λ.&lt;/p&gt;

&lt;h5 id=&#34;sharing-features-for-rpn-and-fast-r-cnn:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Sharing Features for RPN and Fast R-CNN&lt;/h5&gt;

&lt;p&gt;The authors discuss 3 three ways for training networks with features shared:
1. Alternating training:
    Train RPN and Fast-rcnn in turn iteratively.
2. Approximate joint training:
    Train 2 network together by ignore the derivative w.r.t. the proposal boxes’ coordinates that are also network responses, so is approximate.
3. Non-approximate joint training:
    Train 2 network together but don’t ignore the derivative w.r.t. the proposal boxes’ coordinates. However, an RoI pooling layer is not differentiable w.r.t. the box coordinates.
In this paper, the authors adopt a pragmatic 4-step training algorithm in the end:
1. Train the RPN;
2. Train Fast R-CNN using the proposals generated by the step-1 RPN;
3. Use the detector network to initialize RPN training but fix the shared convolutional layers and only fine-tune the layers unique to RPN;
4. Keep the shared convolutional layers fixed and fine-tune the unique layers of Fast R-CNN.&lt;/p&gt;

&lt;h4 id=&#34;experiments:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Experiments&lt;/h4&gt;

&lt;h5 id=&#34;high-accuracy:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;High accuracy&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%205.01.28%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;high-efficience:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;High efficience&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%205.02.33%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;good-performance-of-rpn-compared-to-ss-and-eb:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Good performance of RPN compared to SS and EB&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L10/Screen%20Shot%202016-05-18%20at%205.03.09%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;reference:2dfd18626dee5ce385c6309bd394bdc4&#34;&gt;Reference&lt;/h4&gt;

&lt;p&gt;[1] R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L8_PAPER</title>
      <link>http://mithril-ntu.github.io/L8_PAPER/</link>
      <pubDate>Thu, 05 May 2016 11:10:19 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L8_PAPER/</guid>
      <description>

&lt;h3 id=&#34;iclr-16:46910f901446ff3425731be83673756d&#34;&gt;ICLR’16&lt;/h3&gt;

&lt;h3 id=&#34;deep-compression-compressing-deep-neural-networks-with-pruning-trained-quantization-and-huffman-coding:46910f901446ff3425731be83673756d&#34;&gt;DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING&lt;/h3&gt;

&lt;h4 id=&#34;introduction:46910f901446ff3425731be83673756d&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;It is hard to deploy neural networks into embedding systems as these systems have limited memory and power. To solve this problem, this paper proposes a method called “Deep compression” which is composed of 3 steps: pruning, trained quantization and Huffman coding. It can reduce the storage requirement of neural networks by 35X to 49X without affecting their accuracy.
This pipeline is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%2012.37.17%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;pipeline:46910f901446ff3425731be83673756d&#34;&gt;Pipeline&lt;/h4&gt;

&lt;h5 id=&#34;network-pruning:46910f901446ff3425731be83673756d&#34;&gt;NETWORK PRUNING&lt;/h5&gt;

&lt;p&gt;The main intuition of this step is to remove the redundant connections and keep only the most informative connections.
The authors first learn the connectivity via normal network training, then remove all connections with weights below some threshold and finally retrain the new network to learn final weights.
All the new sparse network architecture is stored in compressed sparse row (CSR) or compressed sparse column (CSC) format. To compress further, the index difference is stored instead of the absolute position, and encode this difference in 8 bits for conv layer and 5 bits for fc layer.
&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%2012.50.00%20PM.png&#34; alt=&#34;&#34; /&gt;
Difference larger than the bound is padded by zero.&lt;/p&gt;

&lt;h5 id=&#34;trained-quantization-and-weight-sharing:46910f901446ff3425731be83673756d&#34;&gt;TRAINED QUANTIZATION AND WEIGHT SHARING&lt;/h5&gt;

&lt;p&gt;In this step, the authors limit the number of effective weights by having multiple connections share the same weight, and then fine-tune those shared weights. Firstly, the authors use K-means for clustering weights:
&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.08.43%20PM.png&#34; alt=&#34;&#34; /&gt;
W = [w_1, w_2, &amp;hellip;, w_n], C= [c_1, c_2, &amp;hellip;, c_k] and  n&amp;gt;&amp;gt;k&lt;/p&gt;

&lt;p&gt;Centroid initialization impacts the quality of clustering and thus affects the network’s prediction accuracy. The authors design 3 initialization methods: Forgy(random), density-based, and linear initialization and then examine them in the following experiments.&lt;/p&gt;

&lt;p&gt;For updating the shared weight, the gradient of them is calculated as:
&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.14.19%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The whole step can be illustrated as below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.14.31%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;huffman-coding:46910f901446ff3425731be83673756d&#34;&gt;HUFFMAN CODING&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.17.06%20PM.png&#34; alt=&#34;&#34; /&gt;
As we can see in the above figure, the probability distributions of quantized weights and the sparse matrix index are highly biased. Experiments show that Huffman coding these non-uniformly distributed values saves 20% 30% of network storage.&lt;/p&gt;

&lt;h4 id=&#34;experiments:46910f901446ff3425731be83673756d&#34;&gt;Experiments&lt;/h4&gt;

&lt;p&gt;In the experiments, training is performed with the Caffe framework. And the authors use 2 dataset: MNIST and ImageNet.&lt;/p&gt;

&lt;h5 id=&#34;lenet-300-100-and-lenet-5-on-mnist:46910f901446ff3425731be83673756d&#34;&gt;LENET-300-100 AND LENET-5 ON MNIST&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.28.41%20PM.png&#34; alt=&#34;&#34; /&gt;
As shown above, we can see that most of the saving comes from pruning and quantization, while Huffman coding gives a marginal gain. Moreover, after compression, there is no loss of accuracy.&lt;/p&gt;

&lt;h5 id=&#34;vgg-16-and-alexnet-on-imagenet:46910f901446ff3425731be83673756d&#34;&gt;VGG-16 and ALEXNET ON IMAGENET&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.32.01%20PM.png&#34; alt=&#34;&#34; /&gt;
Tables shown above show that the 2 large networks can be compressed to a low rate of its original size without impacting accuracy.&lt;/p&gt;

&lt;h5 id=&#34;pruning-and-quantization-working-together:46910f901446ff3425731be83673756d&#34;&gt;PRUNING AND QUANTIZATION WORKING TOGETHER&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.35.00%20PM.png&#34; alt=&#34;&#34; /&gt;
When working individually, as shown in the purple and yellow lines, accuracy of pruned and quantized networks begin to drop significantly when compressed below 8% of its original size. However, when working together, the network can be compressed to 3% of original size with no loss of accuracy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.36.55%20PM.png&#34; alt=&#34;&#34; /&gt;
The figure shows that pruning and quantization don’t hurt each other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.38.13%20PM.png&#34; alt=&#34;&#34; /&gt;
The authors also compare 3 initialization methods and prove that linear initialization gets the best performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.39.54%20PM.png&#34; alt=&#34;&#34; /&gt;
The authors compare three different off-the-shelf hardware: the NVIDIA GeForce GTX Titan X and the Intel Core i7 5930K as desktop processors (same package as NVIDIA Digits Dev Box) and NVIDIA Tegra K1 as mobile processor. As shown above, it turns out that the speed and energy efficiency are improved greatly by deep compression.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.45.21%20PM.png&#34; alt=&#34;&#34; /&gt;
Different aggressiveness of weight sharing and quantization are also compared.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.49.02%20PM.png&#34; alt=&#34;&#34; /&gt;
As quantization adds storage for a codebook, the experiment shows that the overhead of codebook is very small and often negligible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L8/Screen%20Shot%202016-04-28%20at%201.50.00%20PM.png&#34; alt=&#34;&#34; /&gt;
Finally, results of different related works and the authors’ are
shown above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L7_PAPER</title>
      <link>http://mithril-ntu.github.io/L7_PAPER/</link>
      <pubDate>Tue, 19 Apr 2016 16:43:50 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L7_PAPER/</guid>
      <description>

&lt;h3 id=&#34;nips-14:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;NIPS‘14&lt;/h3&gt;

&lt;h3 id=&#34;two-stream-convolutional-networks-for-action-recognition-in-videos:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Two-Stream Convolutional Networks for Action Recognition in Videos&lt;/h3&gt;

&lt;h4 id=&#34;introduction:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;The main contribution of this paper can be divided into 3 parts: &lt;strong&gt;a)&lt;/strong&gt; Propose a new two stream(spatial and temporal streams) ConvNet architecture for action recognition; &lt;strong&gt;b)&lt;/strong&gt; Prove that a ConvNet trained on multi-frame dense optical flow can achieve good performance even if there are little training data; &lt;strong&gt;c)&lt;/strong&gt; Propose multi-task learning which can overcome increase training data and improve the performance on both tasks.&lt;/p&gt;

&lt;p&gt;This 2 stream ConvNet architecture is based on the hypothesis that the human visual cortex contains two pathways: the ventral stream (which performs object recognition) and the dorsal stream (which recognises motion).&lt;/p&gt;

&lt;h4 id=&#34;two-stream-convnet-architecture:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Two stream ConvNet architecture&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%203.24.33%20PM.png&#34; alt=&#34;&#34; /&gt;
The authors divide the video into 2 streams which carry different information and then are processed by their corresponding ConvNets. The softmax output of these two ConvNets are combined by late fusion.&lt;/p&gt;

&lt;h5 id=&#34;spatial-stream-convnet:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Spatial stream ConvNet&lt;/h5&gt;

&lt;p&gt;This ConvNet processes still images extracted from video frames which carry clues about objects and scenes associated with human actions. The author makes good use of the network pre-trainded on the ImageNet challenge dataset to improve this ConvNet.&lt;/p&gt;

&lt;h5 id=&#34;optical-flow-convnets:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Optical flow ConvNets&lt;/h5&gt;

&lt;p&gt;In this ConvNet, the only difference with the spatial ConvNet’s configurations is that the second normalisation layer is removed  from the latter to reduce memory consumption.
The authors configure different inputs for this new model: &lt;strong&gt;Optical flow stacking&lt;/strong&gt;, &lt;strong&gt;Trajectory stacking&lt;/strong&gt; and &lt;strong&gt;Bi-directional optical flow&lt;/strong&gt;, which is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%203.41.26%20PM.png&#34; alt=&#34;&#34; /&gt;
Moreover, to compensate the camera motion, the authors also perform “&lt;strong&gt;Mean flow subtraction&lt;/strong&gt;” —— from each displacement field &lt;strong&gt;d&lt;/strong&gt; the author subtract its mean vector. This also is generally beneficial to perform zero-centering of the network input, as it allows the model to better exploit the rectification non-linearities.&lt;/p&gt;

&lt;p&gt;The authors think that the hand-crafted features in prior art can be obtained from the displacement field input using a single convolutional layer, which means that they can be generalised by their ConvNets, so as to the HMAX models.&lt;/p&gt;

&lt;h4 id=&#34;multi-task-learning:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Multi-task learning&lt;/h4&gt;

&lt;p&gt;Unlike the spatial ConvNet which can be pre-trained on ImageNet Challenge dataset, the amount of the data that can be used in the temporal ConNet is less. In order to increase data, the authors set 2 softmax classification layer on top of the last FC layer for different dataset. The overall training loss is computed as the sum of the individual tasks’ losses. Therefore, the whole ConvNet can make use of different datasets for different tasks.&lt;/p&gt;

&lt;h4 id=&#34;experiments:9c4afc3940f2cf7cbf85c1866eccd57b&#34;&gt;Experiments&lt;/h4&gt;

&lt;p&gt;The architecture of 2 stream ConvNet is based on VGG and, for the spatial ConvNet, pre-trained on ImageNet ILSVRC-2012. The datasets used for evaluation are UCF-101 and HMDB-51.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Firstly&lt;/strong&gt;, the authors compare different settings on the spatial and temporal ConvNet. The results are shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%204.17.33%20PM.png&#34; alt=&#34;&#34; /&gt;
It turns out that fine-tuning on the whole pre-trained model can lead to the best performance, but the improvement is too slightly and  time-consuming. So the authors opt for only training the last layer in following experiments.
On the other hand, for the temporal ConvNet, using the Optical flow stacking as input is a good choice and it also shows that increasing the number of input flows(L) and performing Mean flow subtraction make a great difference.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Secondly&lt;/strong&gt;, the authors compare different methods to increase the training dataset size: (i) fine-tuning a temporal network pre-trained on UCF-101; (ii) adding 78 classes from UCF-101, which are manually selected so that there is no intersection between these classes and the native HMDB-51 classes; (iii) using the multi-task formulation to learn a video representation, shared between the UCF-101 and HMDB-51 classification tasks. The results is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%204.33.41%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Thirdly&lt;/strong&gt; ,different fusion methods for combining the softmax scores of the spatial and temporal ConvNet are tested.
&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%204.35.58%20PM.png&#34; alt=&#34;&#34; /&gt;
Obviously, the combination of the two nets further improves the results, especially when using SVM for fusion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, by comparing with result in prior art, the architecture proposed in this paper shows its competitive performance.
&lt;img src=&#34;http://Mithril-NTU.github.io/L7/Screen%20Shot%202016-04-19%20at%204.37.31%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HD_LBP</title>
      <link>http://mithril-ntu.github.io/HD_LBP/</link>
      <pubDate>Sun, 17 Apr 2016 11:47:29 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/HD_LBP/</guid>
      <description>

&lt;h3 id=&#34;cvpr-13:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;CVPR’13&lt;/h3&gt;

&lt;h3 id=&#34;blessing-of-dimensionality-high-dimensional-feature-and-its-efficient-compression-for-face-verification:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Blessing of Dimensionality: High-dimensional Feature and Its Efficient Compression for Face Verification&lt;/h3&gt;

&lt;h4 id=&#34;apporach:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Apporach&lt;/h4&gt;

&lt;p&gt;This paper has two main contributions:&lt;/p&gt;

&lt;h5 id=&#34;a-show-that-high-dimensionality-is-critical-to-high-performance:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;(A)Show that high dimensionality is critical to high performance&lt;/h5&gt;

&lt;p&gt;The author uses explicit shape regression to locate accurate landmarks on faces and then rectify similarity transformation based on five land- marks (eyes, nose, and mouth corners). With the location of these landmarks, the author extracts multi-scale patches around them and divide these patches into cells. Finally, these cells are coded by some local descriptors.
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.03.37%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;b-propose-a-sparse-projection-method-named-rotated-sparse-regression:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;(B)Propose a  sparse projection method, named rotated sparse regression&lt;/h5&gt;

&lt;p&gt;Firstly, the author adopts PCA for feature dimension reduction and uses some supervised subspace learning methods like LDA or Joint Beyesian to extract discriminative information for face recognition and (potentially) further reduce the dimension.
Secondly, he uses propose a  sparse projection method, named rotated sparse regression, to learn a sparse linear projection.
This RSR(rotated sparse regression) is based on this sparse coding function:
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.12.47%20AM.png&#34; alt=&#34;&#34; /&gt;
By adding a rotation matrix R, this becomes to:
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.12.55%20AM.png&#34; alt=&#34;&#34; /&gt;
Though fixing R and B in turn, we can optimize them iteratively.
Finally, we can get a linear projection matrix B with additional freedom in rotation.
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.17.58%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;experiment:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Experiment&lt;/h4&gt;

&lt;h5 id=&#34;the-high-dimensional-feature-is-better:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;The High-dimensional feature is better&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.24.57%20AM.png&#34; alt=&#34;&#34; /&gt;
The author first compares their results with baseline feature extracted from regular grids to show that sampling at the landmarks leads to comparatively better performance, which indicates sampling at the landmarks effectively reduce the intra-personal geometric variations due to pose and expressions.
Then he compares different scale number and landmark number:
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.27.34%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;large-scale-dataset-favors-high-dimensionality:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Large scale dataset favors high dimensionality&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.28.39%20AM.png&#34; alt=&#34;&#34; /&gt;
The author uses a new and larger dataset named WDRef to evaluate the performance of large scale dataset. It turns out that  high dimensionality plays an even more important role when the size of the training set becomes larger.&lt;/p&gt;

&lt;h5 id=&#34;high-dimensional-feature-with-unsupervised-learning:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;High-dimensional feature with unsupervised learning&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.31.53%20AM.png&#34; alt=&#34;&#34; /&gt;
The high dimension feature also leads to high performance in unsupervised learning.&lt;/p&gt;

&lt;h5 id=&#34;compression-by-rotated-sparse-regression:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Compression by rotated sparse regression&lt;/h5&gt;

&lt;p&gt;By varying the value of 𝜆, the author compares the sparse regression and the rotated sparse regression under different sparsity.
&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.34.41%20AM.png&#34; alt=&#34;&#34; /&gt;
RSR can reduce the cost of linear projection by 100 times with less than 0.1% accuracy drop.&lt;/p&gt;

&lt;h5 id=&#34;comparison-with-feature-selection:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Comparison with Feature Selection&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.35.48%20AM.png&#34; alt=&#34;&#34; /&gt;
The author compare the rotated sparse regression and two feature selection methods: backward greedy and structure sparsity.
This result verifies the effectiveness of the proposed method (RSR). It also indicates that the majority of dimensions in our high-dimensional feature are informative and complementary.&lt;/p&gt;

&lt;h5 id=&#34;comparison-with-the-state-of-the-art:d8e10a38c5169f493ef99a37c63f86f5&#34;&gt;Comparison with the state-of-the-art&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/CVPR13_DongChen/Screen%20Shot%202016-04-17%20at%2011.40.25%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L6_PAPER</title>
      <link>http://mithril-ntu.github.io/L6_PAPER/</link>
      <pubDate>Sun, 10 Apr 2016 17:11:27 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L6_PAPER/</guid>
      <description>

&lt;h3 id=&#34;cvpr-05:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;CVPR‘05&lt;/h3&gt;

&lt;h3 id=&#34;a-bayesian-hierarchical-model-for-learning-natural-scene-categories:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;A Bayesian Hierarchical Model for Learning Natural Scene Categories&lt;/h3&gt;

&lt;h4 id=&#34;introduction:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;The main problem in this paper is to learn and recognize natural scene categories without human annotation. According to previous work, it turns out that the key point to classify scenes is to use some intermediate representations. However, this kind of methods need tedious ,somewhat arbitrary and possibly sub-optimal, manual annotation. So this paper’s author proposes a new method which is incorporated with models used in texture classification. A scene image can be divide into many local regions(described by codewords) and these regions can be clustering to different themes and then to different categories.
The main framework is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.28.57%20PM.png&#34; alt=&#34;&#34; /&gt;
1. Divide training images into different patches and represent these patches as part of different themes
2. Learns the theme distributions over categories as well as the codewords distribution over the themes without supervision, the author’s method is based on LDA.
3. Represent testing images with codewords and classify them to the label that gives the highest likelihood probability&lt;/p&gt;

&lt;h4 id=&#34;approach-details:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;Approach details&lt;/h4&gt;

&lt;h6 id=&#34;model-details:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;Model details&lt;/h6&gt;

&lt;p&gt;The model we need to learn is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.41.25%20PM.png&#34; alt=&#34;&#34; /&gt;
The meaning of notations in the above image is shown in red.&lt;/p&gt;

&lt;p&gt;Based on this model, the joint probability of of a theme mixture π, a set of N themes z, a set of N patches x and the category c is
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.45.39%20PM.png&#34; alt=&#34;&#34; /&gt;
where function Dir() is
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.45.48%20PM.png&#34; alt=&#34;&#34; /&gt;
According to this, we easily know that when x, Θ and β is given, the probability of a scene class c is
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.51.52%20PM.png&#34; alt=&#34;&#34; /&gt;
where the last term is
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.56.33%20PM.png&#34; alt=&#34;&#34; /&gt;
Using Jensen’s inequality, we can bound this log likelihood as following:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.57.15%20PM.png&#34; alt=&#34;&#34; /&gt;
By letting L(γ,φ;θ,β) denote the RHS of the above equation, we have:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%203.57.39%20PM.png&#34; alt=&#34;&#34; /&gt;
where the second term on the RHS of the above equation stands for the Kullback-Leibler distance of two probabil- ity densities.
Through iteratively estimating the variational parameters γ and φ and then estimating the model parameters θ and β in turn, we can maxmize the log likelihood term log p(x|θ, β, c).&lt;/p&gt;

&lt;h5 id=&#34;feature-details:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;Feature details&lt;/h5&gt;

&lt;p&gt;The author extracts local regions from images in several ways:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%204.05.58%20PM.png&#34; alt=&#34;&#34; /&gt;
And then he represents these regions by 2 different features: normalized 11 × 11 pixel gray values and a 128−dim SIFT vector.
The codebook is learned by K-means algorithm.
The comparison result is:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%205.05.54%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;experiments:c72d5234a95e9488aa0b579f888ff00d&#34;&gt;Experiments&lt;/h4&gt;

&lt;p&gt;The dataset used for the experiment contains 13 categories.
The classification result is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%204.12.50%20PM.png&#34; alt=&#34;&#34; /&gt;
We can see that using both the best and second best choices, the mean categorization result is up to 82.3%. In the confusing table, it’s obvious that four indoor categories suffer severe confusion with each other. The main reason is that the distribution of both the themes and the codewords of these four indoor categories is similar:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%204.16.50%20PM.png&#34; alt=&#34;&#34; /&gt;
This also can be proved by their relationship shown in the dendrogram:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%205.04.14%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The author also compares the other things:
&lt;img src=&#34;http://Mithril-NTU.github.io/L6/Screen%20Shot%202016-04-10%20at%205.04.51%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L5_PAPER</title>
      <link>http://mithril-ntu.github.io/L5_PAPER/</link>
      <pubDate>Mon, 21 Mar 2016 23:46:37 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L5_PAPER/</guid>
      <description>

&lt;h3 id=&#34;science-00:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Science’00&lt;/h3&gt;

&lt;h3 id=&#34;nonlinear-dimensionality-reduction-by-locally-linear-embedding:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Nonlinear Dimensionality Reduction by Locally Linear Embedding&lt;/h3&gt;

&lt;h4 id=&#34;abstract:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Abstract&lt;/h4&gt;

&lt;h5 id=&#34;problem:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Problem:&lt;/h5&gt;

&lt;p&gt;How to discover compact representations of high-dimensional data, which can reserve the manifold structure of original data.&lt;/p&gt;

&lt;h5 id=&#34;solution:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Solution:&lt;/h5&gt;

&lt;p&gt;Locally Linear Embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs&lt;/p&gt;

&lt;h4 id=&#34;summary:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Summary&lt;/h4&gt;

&lt;h5 id=&#34;intuition:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Intuition&lt;/h5&gt;

&lt;p&gt;The geometric intuition of LLE is that the author expects each data point and its neighbors to lie on or close to a locally linear patch of the manifold, so he characterize the local geometry of these patches by linear coefficients that reconstruct each data point from its neighbors.&lt;/p&gt;

&lt;h5 id=&#34;methods:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Methods&lt;/h5&gt;

&lt;p&gt;Based on this intuition, the author defines a cost function:
&lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.11.06%20PM.png&#34; alt=&#34;&#34; /&gt;
This cost function subjects to 2 constraints:
(1) enforcing W_ij   0 if X_j does not belong to the set of neighbors of X_i;
(2) &lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.13.30%20PM.png&#34; alt=&#34;&#34; /&gt;
Through minimising the cost function, which is a least-squares problem, we can get weights that reflect intrinsic geometric properties of the data that are invariant to rotations, rescalings, and translations of that data point and its neigh- bors.&lt;/p&gt;

&lt;p&gt;Then the author maps the high dimension data to a low dimension vector representing global internal coordinates on the manifold based on the weights we get above. To get the best result, the author minimises a cost function as following:
&lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.34.36%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The procedures described above can be shown in the figure below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.37.07%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;examples:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Examples&lt;/h5&gt;

&lt;p&gt;Two-dimensional embeddings based on LLE of faces and words are shown:
&lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.39.17%20PM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L5/Screen%20Shot%202016-03-21%20at%2011.39.05%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;advantages:d6a9e0d3e47c951cd76ecbc765d293e7&#34;&gt;Advantages&lt;/h5&gt;

&lt;p&gt;(1) Guarantee the global optimality or convergence;
(2) Involve less free parameters, which means simply tuning;
(3) The optimizations of LLE are especially tractable;
(4) Scales well with the intrinsic manifold dimensionality, d, and do not require adiscretized gridding of the embedding space.
(5) Avoid the need to solve large dynamic programming problems, and tend to accumulate very sparse matrices, whose structure can be exploited for savings in time and space.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L4_PAPER</title>
      <link>http://mithril-ntu.github.io/L4_PAPER/</link>
      <pubDate>Wed, 16 Mar 2016 20:38:14 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L4_PAPER/</guid>
      <description>

&lt;h3 id=&#34;icml-09:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;ICML‘09&lt;/h3&gt;

&lt;h3 id=&#34;online-dictionary-learning-for-sparse-coding:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Online Dictionary Learning for Sparse Coding&lt;/h3&gt;

&lt;h4 id=&#34;abstract:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Abstract&lt;/h4&gt;

&lt;h5 id=&#34;problem:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Problem&lt;/h5&gt;

&lt;p&gt;To learn basis elements, of which we model data vectors as linear combination, especially on large-scale datasets&lt;/p&gt;

&lt;h5 id=&#34;solution:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Solution&lt;/h5&gt;

&lt;p&gt;Propose a online optimization algorithm for dictionary learning based on stochastic approximations
(A proof of convergence is provided.)&lt;/p&gt;

&lt;h4 id=&#34;introduction:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Learning the dictionary instead of using predefined off-the-shelf bases has been shown to dramatically improve signal reconstruction.But, todays’ methods can’t handle large-scale datasets.
To address this problem, the author proposes a method based on stochastic approximation that process one element of the training set at a time. It is shown to be go further and exploit the specific structure of sparse coding.&lt;/p&gt;

&lt;h4 id=&#34;paper-structure:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;Paper structure&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;2. Problem Statement

3. Online Dictionary Learning
3.1. Algorithm Outline
3.2. Sparse Coding
3.3. Dcitionary Update
3.4. Optimizing the Algorithm

4. Convergence Analysis
4.1. Assumptions
4.2. Main Result and Proof Sketches

5. Experimental Validation
5.1. Performance Evaluation
5.2. Application to Inpainting

6. Discussion
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-problem-statement:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;2. Problem Statement&lt;/h4&gt;

&lt;p&gt;Training set X = [x_1,x_2,…,x_n] in R^(m*n).
The empirical cost function:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%203.51.43%20PM.png&#34; alt=&#34;&#34; /&gt;
where D is in R^(m×k).
The author defines l(x,D) as following:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%203.53.11%20PM.png&#34; alt=&#34;&#34; /&gt;
where λ is a l1 regularization parameter to yield a sparse solution for α.
To prevent D from being arbitrarily large, the author constraints D in a convex set C:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%203.56.02%20PM.png&#34; alt=&#34;&#34; /&gt;
In conclusion, the problem is stated as below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%203.57.15%20PM.png&#34; alt=&#34;&#34; /&gt;
As this problem has 2 variables, the author tries to solve the problem by alternating between the 2 variables, minimizing over one while keeping the other one fixed.&lt;/p&gt;

&lt;p&gt;In particular, given a finite training set, one should not spend too much effort on accurately minimizing the empirical cost, since it is only an approximation of the expected cost.
The expected cost is shown as following:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%204.05.02%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To address this problem, the author proposes a new method based on stochastic approximations but exploits the specific structure of the problem to solve it(especially no need to tune the learning rate ρ).
SG update is shown below:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%204.14.17%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-online-dictionary-learning:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;3. Online Dictionary Learning&lt;/h4&gt;

&lt;h5 id=&#34;3-1-algorithm-outline:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;3.1. Algorithm Outline&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%204.41.44%20PM.png&#34; alt=&#34;&#34; /&gt;
D_t is computed by minimizing over C the function:
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%204.42.45%20PM.png&#34; alt=&#34;&#34; /&gt;
This quadratic function acts as a surrogate for f_t, so D_t can be obtained efficiently using D_(t-1) as warm start&lt;/p&gt;

&lt;h5 id=&#34;3-2-sparse-coding:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;3.2. Sparse Coding&lt;/h5&gt;

&lt;p&gt;Because of the high correlation between the columns in D, the author chooses the LARS-Lasso algorithm to get α.&lt;/p&gt;

&lt;h5 id=&#34;3-3-dictionary-update:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;3.3. Dictionary Update&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%204.45.46%20PM.png&#34; alt=&#34;&#34; /&gt;
The algorithm for updating the dictionary uses block-coordinate descent with warm restarts, and one of its main advantages is that it is parameter-free and does not require any learning rate tuning, which can be difficult in a constrained optimization setting.&lt;/p&gt;

&lt;h5 id=&#34;3-4-optimizing-the-algorithm:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;3.4. Optimizing the Algorithm&lt;/h5&gt;

&lt;p&gt;Some practical tricks:
1) Handling Fixed-size Datasets:
If the same signal x is drawn again at time t &amp;gt; t_0, one would like to remove the “old” information concerning x from A_t and B_t —- that is,
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%205.08.31%20PM.png&#34; alt=&#34;&#34; /&gt;
2) Mini-Batch Extension
For efficiency, the author uses mini-Batch(η signals) in each iteration.
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%205.10.38%20PM.png&#34; alt=&#34;&#34; /&gt;
3) Purging the dictionary from Unused Atoms
Replace unused atoms during the optimization by elements of the training set&lt;/p&gt;

&lt;h4 id=&#34;4-convergence-analysis:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;4. Convergence Analysis&lt;/h4&gt;

&lt;h5 id=&#34;4-1-assumptions:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;4.1. Assumptions&lt;/h5&gt;

&lt;p&gt;1) The data admits a bounded probability density p with compact support K.
2) The quadratic surrogate functions fˆ_t are strictly convex with lower-bounded Hessians.
3) A sufficient uniqueness condition of the sparse coding solution is verified.&lt;/p&gt;

&lt;h5 id=&#34;4-2-main-result-and-proof-sketches:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;4.2. Main Result and Proof Sketches&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%208.29.55%20PM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%208.30.06%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;5-experimental-validation:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;5. Experimental Validation&lt;/h4&gt;

&lt;h5 id=&#34;5-1-performance-evaluation:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;5.1. Performance Evaluation&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%208.33.07%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;5-2-application-to-inpainting:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;5.2. Application to Inpainting&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L4/Screen%20Shot%202016-03-16%20at%208.34.40%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;6-discussion:6e9caa20e3502a65896320b3023d9e3d&#34;&gt;6. Discussion&lt;/h4&gt;

&lt;p&gt;Advantage of this method:&lt;/p&gt;

&lt;p&gt;1) Significantly faster than batch alternatives on large datasets&lt;/p&gt;

&lt;p&gt;2) Not require learning rate tuning like regular stochastic gradient descent methods&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L3_PAPER</title>
      <link>http://mithril-ntu.github.io/L3_PAPER/</link>
      <pubDate>Wed, 16 Mar 2016 00:56:44 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L3_PAPER/</guid>
      <description>

&lt;h3 id=&#34;cvpr-11:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;CVPR‘11&lt;/h3&gt;

&lt;h3 id=&#34;iterative-quantization-a-procrustean-approach-to-learning-binary-codes:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Iterative Quantization: A Procrustean Approach to Learning Binary Codes&lt;/h3&gt;

&lt;h4 id=&#34;abstract:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Abstract&lt;/h4&gt;

&lt;h5 id=&#34;problem:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Problem:&lt;/h5&gt;

&lt;p&gt;Learn similarity-preserving binary codes for efficient retrieval  in large scale image collections&lt;/p&gt;

&lt;h5 id=&#34;solution:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Solution:&lt;/h5&gt;

&lt;p&gt;An alternating minimization scheme(ITQ, Iterative Quantization): Finding a a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube
ITQ can be used with unsupervised or supervised data embeddings&lt;/p&gt;

&lt;h4 id=&#34;introduction:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Encode high-dimensional image descriptor as compact binary strings. This codes should have 3 properties: 1) short enough for storing large datasets, 2) map similar images to binary codes with a low Hamming distance, 3) encode new images efficiently
The author’s approach is like following:
PCA -&amp;gt; Apply a random orthogonal transformation(counteract the variance of different PCA directions) —&amp;gt; ITQ for refining the initial orthogonal transformation to minimize quantization error&lt;/p&gt;

&lt;h4 id=&#34;paper-structure:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;Paper structure&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;2. Unsupervised Code Learning 
2.1. Dimensionality Reduction
2.2. Binary Quantization

3. Evaluation of Unsupervised Code Learning 
3.1. Datasets
3.2. Protocols and Baseline Methods
3.3. Results on CIFAR Dataset
3.4. Results on 580000 Tiny Images

4. Leveraging Label Information

5. Discussion
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-unsupervised-code-learning:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;2. Unsupervised Code Learning&lt;/h4&gt;

&lt;p&gt;Procedure:
1) Apply linear dimensionality reduction to the data(PCA)
2) Perform binary quantization&lt;/p&gt;

&lt;h5 id=&#34;2-1-dimensionality-reduction:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;2.1. Dimensionality Reduction&lt;/h5&gt;

&lt;p&gt;To maximize the variance approximately, we get the following objective function:
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-15%20at%2011.53.58%20PM.png&#34; alt=&#34;&#34; /&gt;
This is the same as PCA, so we get W by taking the top c eigenvectors of the data covariance matrix  X^TX&lt;/p&gt;

&lt;h5 id=&#34;2-2-binary-quantization:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;2.2. Binary Quantization&lt;/h5&gt;

&lt;p&gt;To minimize the quantization loss, we get:
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.09.58%20AM.png&#34; alt=&#34;&#34; /&gt;
where ||.||_F is the Frobenius norm and R is some orthogonal c*c matrix.
The author initializes the R as a random orthogonal matrix. Then     adopt the ITQ procedure:
1) Fix R and update B:
Expanding the formulation above, we have
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.24.50%20AM.png&#34; alt=&#34;&#34; /&gt;
Minimizing this is equivalent to maximize:
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.26.09%20AM.png&#34; alt=&#34;&#34; /&gt;
2) Fix B and update R:
a) Compute the SVD of the c*c matrix B^TV as SΩS’^T
b) Let R = S’S^T
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.29.23%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-evaluation-of-unsupervised-code-learning:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;3. Evaluation of Unsupervised Code Learning&lt;/h4&gt;

&lt;h5 id=&#34;3-1-datasets:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;3.1. Datasets&lt;/h5&gt;

&lt;p&gt;1) CIFAR dataset;
2)580000 Tiny images&lt;/p&gt;

&lt;h5 id=&#34;3-2-protocols-and-baseline-methods:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;3.2. Protocols and Baseline Methods&lt;/h5&gt;

&lt;p&gt;Protocols:
1) To evaluate performance of nearest neighbor search using Euclidean neighbors as ground truth
2) To evaluate the semantic consistency of codes produced by different methods by using class labels as ground truth.&lt;/p&gt;

&lt;p&gt;Baseline methods:
1) LSH;
2) PCA-Direct;
3) PCA-RR;
4) SH;
5) SKLSH;
6) PCA-Nonorth&lt;/p&gt;

&lt;h5 id=&#34;3-3-results-on-cifar-dataset:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;3.3. Results on CIFAR Dataset&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.42.06%20AM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.36.03%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;3-4-results-on-580000-tiny-images:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;3.4. Results on 580000 Tiny Images&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.41.03%20AM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.41.08%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;4-leveraging-label-information:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;4. Leveraging Label Information&lt;/h4&gt;

&lt;p&gt;This section shows how to refine the binary codes in a supervised setting using Canonical Correlation Analysis(CCA).
The goal of CCA is to find projection directions w_k and u_k for feature and label vectors to maximize the correlation between the projected data X*w_k and Y*u_k:
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.48.41%20AM.png&#34; alt=&#34;&#34; /&gt;
The author compares his method with a semi-supervised approach(SSH):
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.50.50%20AM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/L3/Screen%20Shot%202016-03-16%20at%2012.51.45%20AM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;5-discussion:f809bb8f2a579f5c8d55fa3650d344a6&#34;&gt;5. Discussion&lt;/h4&gt;

&lt;p&gt;Contributions of this paper:
1) Show that the performance of PCA-based binary coding schemes can be greatly improved by simply rotating the projected data.&lt;/p&gt;

&lt;p&gt;2) Demonstrate an iterative quantization method for refining this rotation that is very natural and effective.&lt;/p&gt;

&lt;p&gt;Limitation:
Use one bit per projected data dimension.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>L2_PAPER</title>
      <link>http://mithril-ntu.github.io/L2_PAPER/</link>
      <pubDate>Mon, 14 Mar 2016 19:23:42 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/L2_PAPER/</guid>
      <description>

&lt;h3 id=&#34;cvpr-10:87f4392be42726eab66708aaabb6a62d&#34;&gt;CVPR‘10&lt;/h3&gt;

&lt;h3 id=&#34;aggregating-local-descriptors-into-a-compact-image-representation:87f4392be42726eab66708aaabb6a62d&#34;&gt;Aggregating local descriptors into a compact image representation&lt;/h3&gt;

&lt;h4 id=&#34;abstract:87f4392be42726eab66708aaabb6a62d&#34;&gt;Abstract&lt;/h4&gt;

&lt;h5 id=&#34;problem:87f4392be42726eab66708aaabb6a62d&#34;&gt;Problem:&lt;/h5&gt;

&lt;p&gt;To find an image search algorithm for large dataset with high accuracy and efficiency and low memory cost&lt;/p&gt;

&lt;h5 id=&#34;solution:87f4392be42726eab66708aaabb6a62d&#34;&gt;Solution:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Aggregating local image descriptors into a vector with limited dimension&lt;/li&gt;
&lt;li&gt;jointly reduce features’ dimension and index them&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;introduction:87f4392be42726eab66708aaabb6a62d&#34;&gt;Introduction&lt;/h4&gt;

&lt;p&gt;Today’s approaches are hard to fulfil 3 constraints: the search accuracy, its efficiency and the memory usage. To overcome this, the authors propose a new approach called “VLAD(Vector of Locally Aggregated Descriptors)”.
It proposes an image representation that provides high search accuracy with reasonable vector dimensionality. Then it jointly optimising the trade-off between the dimensionality reduction and the indexation algorithm.&lt;/p&gt;

&lt;h4 id=&#34;paper-structure:87f4392be42726eab66708aaabb6a62d&#34;&gt;Paper structure&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;2.Image vector representation
2.1.Bag of features
2.2.Fisher kernel
2.3.VLAD

3.From vectors to codes
3.1.Approximate nearest neighbour
3.2.Indexation-aware dimensionality reduction

4.Experiments
4.1.Evaluation datasets and local descriptor
4.2.Image vector representations
4.3.Reduction and indexation
4.4.Compare with the state of the art
4.5.Large scale experiments
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;2-image-vector-representation:87f4392be42726eab66708aaabb6a62d&#34;&gt;2. Image vector representation&lt;/h4&gt;

&lt;h5 id=&#34;2-1-bag-of-features:87f4392be42726eab66708aaabb6a62d&#34;&gt;2.1. Bag of features&lt;/h5&gt;

&lt;p&gt;Extract local features from images —&amp;gt; Group these features into k “visual words”(k-means clustering) —&amp;gt; Represent an image by a weighted and normalised histograms&lt;/p&gt;

&lt;h5 id=&#34;2-2-fisher-kernel:87f4392be42726eab66708aaabb6a62d&#34;&gt;2.2 Fisher kernel&lt;/h5&gt;

&lt;p&gt;Learn a parametric generative model from training data -&amp;gt; Describe a image with the gradient in parameter space which means how the learnt model should be modified to better fit the observed data&lt;/p&gt;

&lt;h5 id=&#34;2-3-vlad:87f4392be42726eab66708aaabb6a62d&#34;&gt;2.3. VLAD&lt;/h5&gt;

&lt;p&gt;1) Use SIFT descriptors &amp;amp; Learn k “visual words” ;
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.36.18%20PM.png&#34; alt=&#34;&#34; /&gt;
2) Compute VLAD；
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.36.36%20PM.png&#34; alt=&#34;&#34; /&gt;
3) v is subsequently L2 -normalized；
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.39.34%20PM.png&#34; alt=&#34;&#34; /&gt;
The whole computation is like following:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%204.40.27%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-from-vectors-to-codes:87f4392be42726eab66708aaabb6a62d&#34;&gt;3. From vectors to codes&lt;/h4&gt;

&lt;p&gt;This problem is divided into 2 steps: 1) a projections that reduces the dimensionality of the vector and 2) a quantisation used to index the resulting vectors.&lt;/p&gt;

&lt;h5 id=&#34;3-1-approximate-nearest-neighbour:87f4392be42726eab66708aaabb6a62d&#34;&gt;3.1. Approximate nearest neighbour&lt;/h5&gt;

&lt;p&gt;ANN is an approach that embeds a vector into a binary space with  excellent accuracy and efficient memory usage. And it provides explicit approximation of the indexed vectors.The author uses the asymmetric distance computation(ADC) variant of this approach.
To find the α nearest neighbours NN_α(x) of x, we just need compute:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.29%20PM.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.44%20PM.png&#34; alt=&#34;&#34; /&gt;
To embed the vector x into a binary space, we first spill it into (x^1, … , x^m) of equal length D/m. Then a product of quantiser is :
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.37%20PM.png&#34; alt=&#34;&#34; /&gt;
The approximation between the original vector y and the quantiser is:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.32.53%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;3-2-indexation-aware-dimensionality-reduction:87f4392be42726eab66708aaabb6a62d&#34;&gt;3.2. Indexation-aware dimensionality reduction&lt;/h5&gt;

&lt;p&gt;The author use PCA for dimensionality reduction. Mapping a vector x ∈ R^D to x’=Mx ∈ R^D’ will lead to information loss:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.46.01%20PM.png&#34; alt=&#34;&#34; /&gt;
With quantisation, this becomes:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.47.14%20PM.png&#34; alt=&#34;&#34; /&gt;
IF D’ is large, then ε_p(x) is limited and ε_q(x) is large. There is a trade-off on the number of D’.&lt;/p&gt;

&lt;p&gt;May here can use some optimisation algorithms.&lt;/p&gt;

&lt;p&gt;In addition, because PCA, the variance of the different components of x’ is not balanced. So the author performs an orthogonal transformation after PCA: X’’ = QX’ = QMX’.
Q is chose in the form of a Householder matrix:
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%206.54.12%20PM.png&#34; alt=&#34;&#34; /&gt;
or is chose as a random orthogonal matrix.&lt;/p&gt;

&lt;h4 id=&#34;4-experiments:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.Experiments&lt;/h4&gt;

&lt;h5 id=&#34;4-1-evaluation-datasets-and-local-descriptor:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.1.Evaluation datasets and local descriptor&lt;/h5&gt;

&lt;p&gt;3 datasets: 1) INRIA Holidays dataset; 2) UKB dataset; 3) 10M images collected from Flickr&lt;/p&gt;

&lt;h5 id=&#34;4-2-image-vector-representations:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.2.Image vector representations&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.02.31%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;4-3-reduction-and-indexation:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.3.Reduction and indexation&lt;/h5&gt;

&lt;p&gt;1) Balancing the variance
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.03.35%20PM.png&#34; alt=&#34;&#34; /&gt;
2) Choice of the projection subspace dimension
&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.03.41%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h5 id=&#34;4-4-compare-with-the-state-of-the-art:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.4.Compare with the state of the art&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.04.16%20PM.png&#34; alt=&#34;&#34; /&gt;
For the same memory usage, this paper’s method outperforms others!&lt;/p&gt;

&lt;h5 id=&#34;4-5-large-scale-experiments:87f4392be42726eab66708aaabb6a62d&#34;&gt;4.5.Large scale experiments&lt;/h5&gt;

&lt;p&gt;&lt;img src=&#34;http://Mithril-NTU.github.io/AMMAI_L2/Screen%20Shot%202016-03-14%20at%207.07.03%20PM.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>http://mithril-ntu.github.io/welcome/</link>
      <pubDate>Sat, 12 Mar 2016 02:29:56 +0800</pubDate>
      
      <guid>http://mithril-ntu.github.io/welcome/</guid>
      <description>

&lt;h3 id=&#34;first-blog:2cc7dc244eed4480e8b46c91e911e96b&#34;&gt;First Blog&lt;/h3&gt;

&lt;p&gt;Hello Everyone!&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://Mithril-NTU.github.io/media/tn.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;example&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

</description>
    </item>
    
  </channel>
</rss>